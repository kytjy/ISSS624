---
title: "Take-home Exercise 2: Applied Spatial Interaction Models - A case study of Singapore public bus commuter flows"
date: "07 December 2023"
date-modified: "last-modified"
editor: visual
toc-depth: 4
execute:
  freeze: auto
  echo: true #all the codes will appear
  eval: true #all the codes will run
  warning: false #dont display if there are any warnings
format: 
  html:
    code-fold: false
    code-overflow: scroll
    code-summary: "Show the code"
    code-line-numbers: true
---

# 1 Overview

Urban mobility, characterized by the daily commute of urban dwellers from homes to workplaces, presents complex challenges for transport operators and urban managers. Traditional approaches to understanding these mobility patterns, such as commuter surveys, are often hindered by high costs, time-consuming processes, and the rapid obsolescence of collected data. However, the digitalization of city-wide urban infrastructures, including public buses, mass rapid transits, and other utilities, alongside the advent of pervasive computing technologies like GPS and SMART cards, offers a new paradigm in tracking and analyzing urban movement.

## Objectives

::: panel-tabset
## Aim

This assignment is driven by two primary motivations. First, despite the growing availability of open data for public use, there is a noticeable gap in applied research demonstrating how these diverse data sources can be effectively integrated and analyzed to inform policy-making decisions. Second, there is a need to showcase how GDSA can be utilized in practical decision-making scenarios.

The core task of this assignment is to conduct a case study that exhibits the potential value of GDSA. By leveraging publicly available data from multiple sources, the goal is to build spatial interaction models that unravel the factors influencing urban mobility patterns, particularly focusing on public bus transit. This exercise aims to bridge the gap between the abundance of geospatially-referenced data and its practical application, thereby enhancing the return on investment in data collection and management, and ultimately supporting informed policy-making in urban mobility.

## Tasks

The specific tasks of this take-home exercise are as follows:

### Geospatial Data Science

-   Derive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the [traffic analysis zone (TAZ)](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf).

-   With reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating *Passenger Volume by Origin Destination Bus Stops* and *Bus Stop Location* from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html). The O-D matrix must be aggregated at the analytics hexagon level

    | Peak hour period             | Bus tap on time |
    |------------------------------|-----------------|
    | Weekday morning peak         | 6am to 9am      |
    | Weekday afternoon peak       | 5pm to 8pm      |
    | Weekend/holiday morning peak | 11am to 2pm     |
    | Weekend/holiday evening peak | 4pm to 7pm      |

-   Display the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).

-   Describe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).

-   Assemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.

-   Compute a distance matrix by using the analytical hexagon data derived earlier.

### Spatial Interaction Modelling

-   Calibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.

-   Present the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)

-   With reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual).
:::

# 2 Loading Packages

::: panel-tabset
## Packages

The following packages will be used for this exercise:

| Package                                                                                                          | Description                                                              |
|---------------------------------------|---------------------------------|
| [**tmap**](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html)                          | For thematic mapping                                                     |
| [**sf**](https://r-spatial.github.io/sf/) & [**sp**]()                                                           | For importing, integrating, processing, and transforming geospatial data |
| [**tidyverse**](https://www.tidyverse.org/)                                                                      | For non-spatial data wrangling                                           |
| [**DT**](https://rstudio.github.io/DT/)                                                                          | For creating and working with html tables                                |
| [**performance**](https://www.rdocumentation.org/packages/ROCR/versions/1.0-1/topics/performance)                | For computing model comparison metrics                                   |
| [**reshape2**](https://seananderson.ca/2013/10/19/reshape/)                                                      | For data transformation between wide and long formats                    |
| [**ggpubr**](https://rpkgs.datanovia.com/ggpubr/) and [**gtsummary**](https://www.danieldsjoberg.com/gtsummary/) | For creating publication quality analytical and summary tables           |
| [**stplanr**](https://github.com/ropensci/stplanr)                                                               | For transport planning and analysis                                      |
| [**knitr**](https://cran.r-project.org/web/packages/knitr/)                                                      | For dynamic report generation                                            |
| [**scales**](https://scales.r-lib.org/)                                                                          | For scaling graphs                                                       |
| [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html)                   | For visualising correlation matrix                                       |
| [**plotly**](https://plotly-r.com/)                                                                              | For interactive and dynamic plots                                        |

## Code

The code chunk below, using `p_load` function of the [**pacman**](https://cran.r-project.org/web/packages/pacman/pacman.pdf) package, ensures that packages required are installed and loaded in R.

```{r}
pacman::p_load(tmap, sf, sp, 
               tidyverse, DT, performance, 
               reshape2, ggpubr, stplanr,
               knitr, scales, corrplot, 
               gtsummary, plotly)
```
:::

# 3 Data Preparation

For the purpose of this assignment, the following data will be used:

|     |            |                                                                                                                                                                                                                                                                                                                   |                |            |                                                                                                          |
|------------|------------|------------|------------|------------|------------|
|     | **Type**   | **Name**                                                                                                                                                                                                                                                                                                          | **As of Date** | **Format** | **Source**                                                                                               |
| 1   | Aspatial   | Passenger Volume by Origin Destination Bus Stops                                                                                                                                                                                                                                                                  | Oct 2023       | .csv       | [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en/dynamic-data.html)                        |
| 2   | Aspatial   | School Directory and Information (General information of schools)                                                                                                                                                                                                                                                 | Mar 2022       | .csv       | [Data.gov.sg](https://beta.data.gov.sg/collections/457/datasets/d_688b934f82c1059ed0a6993d2a829089/view) |
| 3   | Aspatial   | HDB Property Information (Geocoded)                                                                                                                                                                                                                                                                               | Sep 2021       | .csv       | Courtesy of Prof T. S. Kam                                                                               |
| 4   | Geospatial | Bus Stop Location                                                                                                                                                                                                                                                                                                 | Jul 2023       | .shp       | [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en/static-data.html)                         |
| 5   | Geospatial | Train Station Exit Point                                                                                                                                                                                                                                                                                          | Aug 2023       | .shp       | [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en/static-data.html)                         |
| 6   | Geospatial | Master Plan 2019 Subzone Boundary                                                                                                                                                                                                                                                                                 | 2019           | .shp       | Courtesy of Prof T.S. Kam                                                                                |
| 7   | Geospatial | Business (incl. industrial parks), FinServ, Leisure&Recreation and Retails (Geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets compiled for urban mobility study) |                | .shp       | Courtesy of Prof T.S. Kam                                                                                |

## 3.1 O-D Data

::: panel-tabset
## Importing csv

*Passenger Volume by Origin Destination Bus Stops* dataset for October 2023, downloaded from LTA DataMall by using `read_csv()` or **readr** package.

```{r}
odbus <- read_csv("data/aspatial/origin_destination_bus_202310.csv")
```

## Attributes

`glimpse()` of the **dplyr** package allows us to see all columns and their data type in the data frame.

```{r}
glimpse(odbus)
```

**Observations:**

-   There are 7 variables in the odbus tibble data, they are:
    -   YEAR_MONTH: Month in which data is collected
    -   DAY_TYPE: Weekdays or weekends/holidays
    -   TIME_PER_HOUR: Hour which the passenger trip is based on, in intervals from 0 to 23 hours
    -   PT_TYPE: Type of public transport, i.e. bus
    -   ORIGIN_PT_CODE: Origin bus stop ID
    -   DESTINATION_PT_CODE: Destination bus stop ID
-   TOTAL_TRIPS: Number of trips We also note that values in ORIGIN_PT_CODE and DESTINATON_PT_CODE are in numeric data type. These should be in factor data type for further processing and georeferencing.

`as.factor()` can be used to convert the variables ORIGIN_PT_CODE and DESTINATON_PT_CODE from numeric to categorical data type. We use `glimpse()` again to check the results.

```{r}
odbus$ORIGIN_PT_CODE <- as.factor(odbus$ORIGIN_PT_CODE)
odbus$DESTINATION_PT_CODE <- as.factor(odbus$DESTINATION_PT_CODE)

glimpse(odbus)
```

Note that both of them are in factor data type now.

## Extracting Study Data

In our study, we would like to analyse the 1 of the peak hour periods identified. We will be analysing the **Weekday Morning** peak periods thereafter. Therefore, we can employ a combination of the following functions to obtain the relevant data:

Summary of the functions used as follow:

-   `filter()`: Retains rows that satisfies our condition (i.e. Weekday Morning peak period)

-   `select()` of **dplyr** package: Retains the desired variables for further analysis.

-   `group_by()` and `summarise()`: Aggregates the total trips at each combination of origin bus stop, destination bus stop, and peak period.

```{r}
WDMpeak <- odbus %>%
  filter(DAY_TYPE=="WEEKDAY" & (TIME_PER_HOUR >= 6 & TIME_PER_HOUR <= 9)) %>% 
  dplyr::select(5:7)  %>% 
  group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %>% 
  summarise(TRIPS=sum(TOTAL_TRIPS))
```

Let's check the output using the `glimpse()` function of dplyr.

```{r}
glimpse(WDMpeak)
```
:::

## 3.2 Geospatial Data

For the purpose of this exercise, three geospatial data will be used. They are:

-   **MPSZ-2019**: This data provides the sub-zone boundary of URA Master Plan 2019, it helps us define the geographical boundary of Singapore.
-   **BusStop**: This data provides the location of bus stop as at Jul 2023.
-   **Analytical hexagon**: Hexagonal grids of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone.

In this section, we import the shapefiles into RStudio using `st_read()` function of sf package. `st_transform()` function of **sf** package is used to transform the projection to coordinate reference system (CRS) 3414, which is the EPSG code for the SVY21 projection used in Singapore.

::: panel-tabset
## Master Plan Subzone

```{r}
mpsz <- st_read(dsn="data/geospatial",                   
                layer="MPSZ-2019")%>%   
  st_transform(crs = 3414)
```

In the code chunk below, `tm_shape()` of **tmap** package is used to define the input data (i.e mpsz) and `tm_polygons()` is used to draw the planning subzone polygons.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")

tm_shape(mpsz) +
  tm_polygons(col='#C2D3CC', border.alpha = 0.1)
```

## Bus Stop

```{r}
busstop <- st_read(dsn = "data/geospatial",
                   layer = "BusStop") %>% 
  st_transform(crs = 3414)

```

Busstop represents sf point objects for 5161 bus stop in Singapore.

To visualise the points of the bus stops, we can use `tm_shape()` of tmap package with each bus stop point displayed as dots. *tmap_mode* allows us to view static maps with `plot` and interactive maps with `view`.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons(col='#C2D3CC', border.alpha = 0.1, alpha = 0.3) +
tm_shape(busstop) +
  tm_dots()+
tm_view(set.zoom.limits = c(11,14)) # to fix the map extent, so cannot zoom in too much
```

Before proceeding, let's check if there are any duplicated bus stops in the dataset.

```{r}
bs_dupes <- busstop %>%
  group_by(BUS_STOP_N) %>%
  filter(n() > 1) %>%
  ungroup() %>%
  arrange(BUS_STOP_N)

knitr::kable(bs_dupes)

```

The results displayed 16 pairs of duplicated *BUS_STOP_N*, with each pair showing a different geometry point for the same bus stop number. This could potentially suggest that these are temporary bus stops. In that case, it would be prudent to retain only one of them, as conventionally, only one bus stop is used at a time.

```{r}
busstop <- busstop %>%
  distinct(BUS_STOP_N, 
           .keep_all = TRUE)

```

Notice that the number of bus stops has dropped from 5161 to 5145.

Note from the choropleth map that there are 5 bus stops located outside Singapore, they are bus stops **46239, 46609, 47701, 46211, and 46219**. The code chunk below uses *filter()* to exclude the 5 bus stops outside Singapore.

```{r}
busstop <- busstop %>%   
  filter(!BUS_STOP_N %in% c(46239, 46609, 47701, 46211, 46219))
```

Notice that the number of bus stops has dropped from 5145 to **5140**.

## Analytical Hexagon

A hexagonal grid is used to represent the traffic analysis zones, which helps to model travel demand through capturing the spatial aspects of trip origins and destinations.

**Step 1: Create Hexagonal Grids**

We first create a hexagonal grid layer of 375m (refers to the perpendicular distance between the centre of the hexagon and its edges) with [`st_make_grid`](https://r-spatial.github.io/sf/reference/st_make_grid.html), [`st_sf`](https://r-spatial.github.io/sf/reference/sf.html) to convert the grid into an sf object with the codes below, and `row_number()` to assign an ID to each hexagon.

::: {.callout-note collapse="true"}
## *st_make_grid* Arguments

*st_make_grid* function is used to create a grid over a spatial object. It takes 4 arguments, they are:

-   x: sf object; the input spatial data

-   cellsize: for hexagonal cells the distance between opposite edges in the unit of the crs the spatial data is using. In this case, we take cellsize to be 375m \* 2 = 750m

![](images/hex.PNG){width="276"}

-   what: character; one of: `"polygons"`, `"corners"`, or `"centers"`
-   square: indicates whether you are a square grid (TRUE) or hexagon grid (FALSE)
:::

```{r}
area_hexagon_grid = st_make_grid(busstop, 
                                 cellsize= 750, 
                                 what = "polygons", 
                                 square = FALSE,
                                 crs = 3414) %>% 
  st_sf() %>% 
  mutate(grid_id = row_number())
  
```

**Step 2: Remove grids with no bus stops**

We count the number of bus stops in each grid and retain only the grids with bus stops using the code chunks below.

[`st_intersects`](https://postgis.net/docs/ST_Intersects.html) is used to identify the bus stops falling inside each hexagon, while [`lengths`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/lengths) returns the number of bus stops inside each hexagon.

```{r}
# Create a column containing the count of bus stops in each grid
area_hexagon_grid$busstops = lengths(
  st_intersects(
    area_hexagon_grid, 
    busstop))

# Retain hexagons with bus stops
area_hexagon_grid = filter(area_hexagon_grid, 
                           busstops > 0)

```

Notice that 831 hexagons have been created.

**Step 3: Check & Visualise**

```{r}
sum(area_hexagon_grid$busstops, na.rm = TRUE)
```

Note that there are **5140** bus stops, which tallies to the 5140 from the **Busstop** shape file after deducting for the 5 bus stops outside Singapore boundary and the 16 duplicates, suggesting that the hexagons have managed to capture all expected bus stops.

In the bar chart below, it is evident that the distribution of bus stops per hexagon is right-skewed. While one hexagon contains as many as 19 bus stops, the majority have fewer than 10 bus stops.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

ggplot(area_hexagon_grid, 
       aes(x= as.factor(busstops)))+   
  geom_bar()+   
  ggtitle("No. of Bus Stops per Hexagon") +
  geom_text(aes(label = after_stat(count)), 
            stat = "count", 
            vjust = -0.5, 
            colour = "black")+
  labs(x= "No. of Bus Stops", y = "Count")

```

Lastly, using `tm_shape` from **tmap** package, we can quickly visualise the results of the hexagon grids we have created.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode ("plot")
tm_shape(area_hexagon_grid)+
  tm_fill(
    col = "busstops",
    palette = "Blues",
    style = "quantile",
    title = "Number of Bus Stops",
    id = "grid_id",
    showNA = FALSE,
    alpha = 0.6,
    popup.format = list(
      grid_id = list(format = "f", digits = 0)
    )
  )+
  tm_borders(col = "grey40", lwd = 0.7)
```
:::

## 3.3 Geospatial Data Wrangling

### 3.3.1 Combining Busstop and Hexagons

Code chunk below populates the grid ID (i.e. grid_id) of **area_hexagon_grid** sf data frame into **busstop** sf data frame using the following functions:

-   `st_intersection()` is used to perform point and polygon overly and the output will be in point sf object.

-   `select()` of **dplyr** package is then use to retain preferred variables from the data frames.

-   `st_stop_geometry()` removes the geometry data to manipulate it like a regular dataframe using `tidyr` and `dplyr` functions

```{r}
bs_wgrids <- st_intersection(busstop, area_hexagon_grid) %>% 
  dplyr::select(BUS_STOP_N,BUS_ROOF_N,LOC_DESC, grid_id) %>% 
  st_drop_geometry
```

Before we proceed, let's perform a duplicates check on **bs_wgrids**.

```{r}
duplicate <- bs_wgrids %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()

duplicate
```

Results showed that there are no duplicated records.

### 3.3.2 Populate Passenger Volume data with Grid IDs

Next, we are going to append the Grid IDs based on **origin** bus stops from **bs_wgrids** data frame onto **WDMpeak** data frame. But before that, ensure that *BUS_STOP_N* of **bs_wgrids** is also in factor data type.

```{r}
bs_wgrids$BUS_STOP_N  <- as.factor(bs_wgrids$BUS_STOP_N)


od_data <- left_join(WDMpeak , bs_wgrids,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>% 
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_GRID = grid_id,
         ORIGIN_DESC = LOC_DESC,
         DESTIN_BS = DESTINATION_PT_CODE)
```

Next, we will update od_data data frame with the Grid IDs of **destination** bus stops.

```{r}
od_data <- left_join(od_data , bs_wgrids,
            by = c("DESTIN_BS" = "BUS_STOP_N")) %>% 
           rename(DESTIN_GRID = grid_id,
                  DESTIN_DESC = LOC_DESC)

glimpse(od_data)
```

The code chunk below allows us to check for duplicates to prevent double counting. The results indicate that there are no duplicates found.

```{r}
duplicate2 <- od_data %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()

duplicate2
```

Next, the code chunk below removes rows with missing data using `drop_na()` and aggregates the total passenger trips at each origin-destination grid level with `group_by()` and `summarise()`. *ORIGIN_nBS* and *DESTIN_nBS* counts the number of bus stops, while *ORIGIN_DESC* and *DESTIN_DESC* provides the descriptions of each of the bus stops at origin and destination grids respectively.

```{r}
od_data <- od_data %>%
  drop_na() %>%
  group_by(ORIGIN_GRID, DESTIN_GRID) %>%
  summarise(MORNING_PEAK = sum(TRIPS),
            ORIGIN_nBS = n_distinct(ORIGIN_BS),
            ORIGIN_DESC = str_c(unique(ORIGIN_DESC), collapse = ", "),
            DESTIN_nBS = n_distinct(DESTIN_BS),
            DESTIN_DESC = str_c(unique(DESTIN_DESC), collapse = ", ")) %>%
  ungroup()
```

Our resulting OD Matrix organises the commuter flows for weekday morning peak period in a column-wise format, with *origin_grid* representing the ***from*** and *destin_grid* representing the ***to***. There are a total of 65,559 unique origin grid to destination grid combinations.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

DT::datatable(od_data,
              options = list(pageLength = 5),
              rownames = FALSE)
```

# 4 Visualising Spatial Interaction

Origin-destination flow maps are a popular option to visualise connections between different spatial locations. It reflects the relationships/flows between locations and are created by monitoring movements. In our analysis, we can use OD flows to identify the patterns of bus ridership during weekday mornings.

## 4.1 Identifying Inter- & Intra-Zonal Flows

Intrazonal travels are considered localised and short duration trips within a transportation analysis zone (i.e. within a hexagon). For our analysis, we will be separating them.

To do that, we create a new column in the od_data dataframe called *Zone_Type* using `mutate()` that labels each row as either "Intrazone" or "Interzone" based on whether the *ORIGIN_GRID* and *DESTIN_GRID* are the same or different.

```{r}
od_data <- od_data %>%
  mutate(Zone_Type = ifelse(ORIGIN_GRID == DESTIN_GRID, "intrazone", "interzone"))

```

There are 623 combinations of intra-zonal travels and 64,936 combinations of inter-zonal travels during the weekday morning peak period based on our dataset.

```{r}
table(od_data$Zone_Type)
```

## 4.2 Interzonal OD Flow Distribution

In the code chunk below, we use `filter()` to identify the inter-data before using `summary()` to evaulate the distribution of data.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

od_interzone <- od_data %>%
  filter(Zone_Type == "interzone")

quantile(od_interzone$MORNING_PEAK, 
         probs = seq(0, 1.0, by = .05))
```

From the summary statistics provided above, we observe that the minimum number of passenger trips for each interzonal combination of origin and destination bus stop is 1. The maximum number of trips recorded is 77,433 passengers, noted during the weekday morning peak period. Additionally, the 90th percentile for passenger trips stands at 174, indicating a highly right-skewed distribution.

## 4.2 Creating Interzonal Flow Lines

Desire lines visually represent the connections between originating and destination hexagons using straight lines. The [`od2line()`](https://docs.ropensci.org/stplanr/reference/od2line.html) function of **stplanr** package is utilized to create these lines. The width of each desire line is proportional to number of passenger trips, i.e. thicker lines would represent higher ridership.

```{r}
# Creating centroids representing desire line start and end points
flowLine <- od2line(flow = od_interzone, 
                    zones = area_hexagon_grid,
                    zone_code = "grid_id")
```

Since there are 65,559 different flow lines resulting from combinations of origin to destination hexagons, an excess of intersecting lines can cause visual clutter and obscure analysis. Considering that the 90th percentile is 658, we will focus on inter-zonal flows with the top 10% of ridership.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("view")
tmap_options(check.and.fix = TRUE)

#tm_basemap("OneMapSG.Grey") +
tm_basemap("OpenStreetMap") +
tm_shape(mpsz) +
  tm_polygons(col='#C2D3CC', border.alpha = 0.1, alpha = 0.3) +
tm_shape(area_hexagon_grid) +
  tm_fill(alpha=0.7, 
          palette="RdBu") + 
  tm_borders(alpha = 0.5)+
flowLine %>%  
  filter(MORNING_PEAK >= 659) %>% 
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.3,
           lines.lwd = "all",
           popup.vars = c("No. of Bus Stops at Origin: " = "ORIGIN_nBS",
                         "Descriptions of Bus Stops at Origin: " = "ORIGIN_DESC",
                         "No. of Bus Stops at Destination: " = "DESTIN_nBS",
                         "Descriptions of Bus Stops at Destination: " = "DESTIN_DESC",
                         "No. of Passenger Trips :" = "MORNING_PEAK"))
#tm_view(set.zoom.limits = c(11,14))
```

The map reveals that Yew Tee, Woodlands, and Yishun dominate inter-zonal bus ridership during weekday mornings, with notably wider desire lines. Key routes include travel within Yew Tee, between Woodlands Checkpoint and Woodlands MRT Station, as well as within Woodlands and Yishun. Interestingly, though broad, these desire lines are relatively short, often indicating bus travel to neighboring hexagons. This suggests a higher demand for feeder bus services in these areas during weekday mornings. Areas such as Boon Lay, Bedok, Choa Chu Kang, Clementi, Tampines, Pasir Ris, and Serangoon also display high concentrations and variations of desire lines with neighboring hexagons, indicating higher ridership within these areas.

Furthermore, longer desire lines between the North and East (i.e., Woodlands and Changi) suggest passengers' willingness to travel longer distances to their destinations.

While OD flows provide valuable insights by quickly visualizing travel patterns, it is beneficial to complement them with other forms of analysis, such as spatial interaction models, for a more comprehensive understanding of the factors affecting urban commuting flow.

# 5 Computing Distance Matrix

A distance matrix is a two-dimensional array containing the distances between different locations. In our analysis, we can use a distance matrix to calculate the distance passengers are willing to travel by bus to get to their destinations.

## 5.1 Converting from sf data.table to SpatialPolygonsDataFrame

Firstly, [`as.Spatial()`](https://r-spatial.github.io/sf/reference/coerce-methods.html) will be used to convert *area_hexagon_grid* from sf tibble data frame to SpatialPolygonsDataFrame of sp object as shown in the code chunk below.

```{r}
hexgrid_sp <- as(area_hexagon_grid, "Spatial")
hexgrid_sp
```

## 5.2 Computing Distance Matrix

Next, [`spDists()`](https://www.rdocumentation.org/packages/sp/versions/2.1-1/topics/spDistsN1) of sp package will be used to compute the Euclidean distance between the centroids of the planning subzones. *spDists* returns a full matrix of distances in the metric of the points if longlat=FALSE, or in kilometers if longlat=TRUE. With 831 hexagons, the return results will produce a 831 by 831 matrix of distance between each hexagon.

```{r}
dist <- spDists(hexgrid_sp, 
                longlat = FALSE)

head(dist, n=c(6, 6))
```

The resulting output is a matrix object class.

Note that column headers and row headers are not labeled with the grid IDs, in the next step, we rename the headers for better clarity.

```{r}
grid_id <- area_hexagon_grid$grid_id

colnames(dist) <- paste0(grid_id)
rownames(dist) <- paste0(grid_id)

head(dist, n=c(6, 6))
```

Notice that the column and row names have been updated to the grid IDs.

## 5.3 Pivoting Distance Value by Grid ID

Next, we will pivot the distance matrix into a long table by using the row and column grid IDs using [`melt()`](https://www.rdocumentation.org/packages/reshape2/versions/1.4.4/topics/melt) of the **reshape2** package, as shown in the code chunk below.

```{r}
distPair <- melt(dist) %>%
  rename(dist = value,
         orig = Var1,
         dest = Var2)

head(distPair, 5)
```

Notice that the within-zone distance is 0.

## 5.4 Updating Intra-Zonal Distances

In this section, we are going to append a constant value to replace the intra-zonal distance of 0.

First, we will select and find out the minimum value of the distance by using `summary()`.

```{r}
distPair %>%
  filter(dist > 0) %>%
  summary()
```

Next, an arbitrary constant distance value of 100m is added into intra-zones distance

```{r}
distPair$dist <- ifelse(distPair$dist == 0,
                        100, 
                        distPair$dist)
```

The code chunk below will be used to check the result using `summary()`.

```{r}
summary(distPair)
  
```

## 5.5 Combining passenger volume data with distance value

Let's convert the origin and destination grid data in *od_data* and *distPair* into factor data type before we combine passenger volume data from **od_data** and distance from **distPair** using `left_join()`.

```{r}
od_data$ORIGIN_GRID  <- as.factor(od_data$ORIGIN_GRID)
od_data$DESTIN_GRID  <- as.factor(od_data$DESTIN_GRID)

distPair$orig  <- as.factor(distPair$orig)
distPair$dest  <- as.factor(distPair$dest)

flow_data <- od_data %>%
  left_join (distPair,
             by = c("ORIGIN_GRID" = "orig",
                    "DESTIN_GRID" = "dest"))

glimpse(flow_data)
```

## 5.6 Distance Distribution

```{r}
#| code-fold: true
#| code-summary: "Show the code"

quantile(flow_data$dist, 
         probs = seq(0, 1.0, by = .1))
```

From the summary statistics above, the minimum number of passenger trips for each combination of origin and destination bus stop is 100m, which is the arbitrary intrazonal travel distance we have set. The maximum observed is 24,784m.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Extract column
distWDM_distance <- flow_data$dist
# Calculate mean 
distWDM_distance_mean <- mean(distWDM_distance)

ggplot(
    data = data.frame(distWDM_distance),
    aes(x = distWDM_distance)
  ) +
  geom_histogram(
    bins = 20, 
    color = "#FFFCF9", 
    fill = "black",
    alpha = .3
  ) +
  # Add line for mean
  geom_vline(
    xintercept = distWDM_distance_mean, 
    color = "#595DE5", 
    linetype = "dashed", 
    linewidth = 1
  ) +
  scale_x_continuous(breaks = pretty(distWDM_distance, n = 10))+
  # Add line annotations
  annotate(
    "text", 
    x = 9000, 
    y = 7500,
    label = paste("Mean =", round(distWDM_distance_mean, 3)),
    color = "#595DE5",
    size = 3
  ) +
  labs(
    title = "Weekday Morning Peak",
    x = "Distance of Bus Trips",
    y = "Frequency"
  ) 
```

## 5.6 Visualise Flow Lines

Since the 90th percentile of the distance traveled is 12,346.558m, we will filter the data for distances greater than 12,347m to analyse the top 10% of the longest distances traveled.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Creating centroids representing desire line start and end points
flowLine2 <- od2line(flow = flow_data, 
                    zones = area_hexagon_grid,
                    zone_code = "grid_id")

tmap_mode("view")
tmap_options(check.and.fix = TRUE)

#tm_basemap("OneMapSG.Grey") +
tm_basemap("OpenStreetMap") +
tm_shape(mpsz) +
  tm_polygons(alpha = 0) +
  tm_borders(alpha = 0.5)+
tm_shape(area_hexagon_grid) +
  tm_fill(alpha=0.7, 
          palette="RdBu") + 
  tm_borders(alpha = 0.5)+
flowLine2 %>%  
  filter(dist >= 12347) %>% 
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.3,
           lines.lwd = "all",
           popup.vars = c("No. of Bus Stops at Origin: " = "ORIGIN_nBS",
                         "Descriptions of Bus Stops at Origin: " = "ORIGIN_DESC",
                         "No. of Bus Stops at Destination: " = "DESTIN_nBS",
                         "Descriptions of Bus Stops at Destination: " = "DESTIN_DESC",
                         "No. of Passenger Trips :" = "MORNING_PEAK"))
#tm_view(set.zoom.limits = c(11,14))
```

The plot below reveals that longer travel distances predominantly occur between North and North-East regions (i.e., Woodlands/Yishun and Punggol), North and East (Woodlands and Tampines), West and South (Choa Chu Kang/Bukit Panjang and the Town area), and North and South (Yishun and the Town area). These patterns may reflect commuting trends between residential and commercial areas, suggesting that significant portion of the population undertake considerable daily commutes.

The sparseness of flow lines seen in the West-end also ascertains the trends noted in our previous Take-Home Exercise 1, where low-low autocorrelation were noted in that area.

In the subsequent sections, we explore the potential factors that could attract or propel passengers to travel by bus from one location to another. This exploration will include examining various urban elements such as the proximity to key amenities like schools, shopping centers, and employment hubs. Understanding these elements can provide valuable insights into improving public transportation systems and urban planning strategies.

# 6 Preparing Origin and Destination Attributes

The following information is used to derive propulsive/attractiveness variables:

1.  *Business*, *FinServ*, *Leisure&Recreation* and *Retails* are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets.

2.  Schools: This data set contains directory and general information of schools in Singapore, obtained from data.gov.

3.  HDB: This data set is the geocoded version of *HDB Property Information* data from data.gov. The data set is prepared using September 2021 data.

::: panel-tabset
## Train Station Exits

**trainstationexits** contains the MRT station names and exits along with their respective **point** geometries in CRS SVY21.

Train stations exits reflect the intermodal connections with bus stops. In the context of **attractiveness**, these stations can be seen as destinations that attract passengers, including those who might transit to/from these stations by bus. The data can also indicate the **propulsiveness** aspect -- how these stations act as origins for passengers who leave the MRT stations and then proceed to their final destinations via other modes of transportation like buses.

### Step 1: Import shapefile

*st_read()* function of the sf package enables us to import the file into RStudio.

```{r}
trainstationexits <- st_read(dsn = "data/geospatial",
                   layer = "Train_Station_Exit_layer") %>% 
  st_transform(crs = 3414)
```

Notice there are 565 train station exits in total.

A quick check for duplicates revealed that there are cases where same station names and exit codes have different geometries. In the absence of further information, it is prudent to retain all these details, as they might represent cases where lifts and escalators at the exits are located at different points.

```{r}
trainstationexits %>%
  group_by(stn_name, exit_code) %>%
  filter(n() > 1) %>%
  ungroup() %>%
  arrange(stn_name, exit_code) %>% 
  kable()

```

### Step 2: Point-in-Polygon Count Process

Next, we will count the number of train station exits located inside each hexagon and include this as a variable in **area_hexagon_grid** table.

```{r}
area_hexagon_grid$`COUNT_TRAINSTATIONEXIT`<- lengths(
  st_intersects(
    area_hexagon_grid, trainstationexits))

sum(area_hexagon_grid$COUNT_TRAINSTATIONEXIT)
```

The 5 train station exits not accounted for could be in areas outside hexagons where there are no bus stop.

Summary statistics indicate that a maximum of 13 train station exits are located within a single hexagon, while at least 75% of the hexagons do not contain any exits.

```{r}
summary(area_hexagon_grid$COUNT_TRAINSTATIONEXIT)
```

Let's visualise where the bus stops are located, alongside the top 5% of ridership, to see if we can identify any discernible patterns.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons(col='#C2D3CC', border.alpha = 0.1, alpha = 0.3) +
tm_shape(area_hexagon_grid) +
  tm_fill(
    col="COUNT_TRAINSTATIONEXIT",
    style="pretty",
    palette="Blues",
    alpha=0.7) +
  tm_borders(alpha = 0.5) +
flowLine %>%  
  filter(MORNING_PEAK >= 1399) %>% 
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.1)+
    tm_layout(legend.text.size = 0.6,
              legend.title.size=0.6)
  
```

## Business

**business** contains the details of various businesses from SMEs to bigger groups like Pan Pacific, as well as the respective **point** geometries in CRS SVY21.

Businesses serve as significant **attractors** in a city. They draw people to these locations, primarily for work purposes. The presence and density of businesses in an area can significantly influence the flow of commuters, making it a measure of attractiveness.

### Step 1: Import shapefile

```{r}
#| code-fold: true
#| code-summary: "Show the code"
business <- st_read(dsn = "data/geospatial",
                      layer = "Business") %>%
          st_transform(crs = 3414)
```

Note that there are 6550 business in our dataset in total.

Our duplicate check showed KEPPEL DISTRIPARK is included twice in the dataset. We remove this to prevent double-counting.

```{r}
business %>%
  group_by_all() %>%
  filter(n() > 1) %>%
  ungroup() %>%
  arrange(POI_NAME) %>% 
  kable()
```

```{r}
business <- unique(business)
```

Note that the number of records for **business** is has decreased from 6550 to 6549.

### Step 2: Point-in-Polygon Count Process

Next, we will count the number of business in each hexagon and include this as a variable in **area_hexagon_grid** table.

```{r}
area_hexagon_grid$`COUNT_BIZ`<- lengths(
  st_intersects(
    area_hexagon_grid, business))
```

Summary statistics below show that less than half of the hexagons do not contain any businesses, while in contrast, a single hexagon houses as many as 97 businesses.

```{r}
summary(area_hexagon_grid$COUNT_BIZ)
```

Let's visualise where the businesses are located, alongside the top 5% of ridership, to see if we can identify any discernible patterns.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons(col='#C2D3CC', border.alpha = 0.1, alpha = 0.3) +
tm_shape(area_hexagon_grid) +
  tm_fill(
    col="COUNT_BIZ",
    style="pretty",
    palette="Blues",
    alpha=0.7) +
  tm_borders(alpha = 0.5) +
flowLine %>%  
  filter(MORNING_PEAK >= 1399) %>% 
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.1)+
    tm_layout(legend.text.size = 0.6,
              legend.title.size=0.6)
  
```

No consistent patterns are evident when comparing the number of businesses in a grid with the top 5% of ridership. In the far West, despite a significant presence of businesses, the passenger volume does not rise correspondingly. In contrast, areas like Changi, Woodlands, the Central Business District, and Geylang, which have higher numbers of business units, also experience greater passenger volumes.

## Financial Centres

**finserv** contains the details of various financial centres, as well as the respective **point** geometries in CRS SVY21.

Financial service locations often represent significant employment centers, especially in urban and commercial areas. Many people travel to these locations for work, making them important **attractors** during morning peak hours. In addition, financial services typically adhere to standard office hours, which aligns well with the morning peak period of bus ridership, as a large proportion of employees would be traveling to work during this time.

### Step 1: Import shapefile

```{r}
#| code-fold: true
#| code-summary: "Show the code"
finserv <- st_read(dsn = "data/geospatial",
                      layer = "FinServ") %>%
          st_transform(crs = 3414)
```

Note that there are 3320 locations of financial services in our dataset in total.

Similarly, we remove duplicates by retaining the unique observations to prevent double-counting. This reduces the number of financial services locations from 3320 to 3058.

```{r}
finserv <- unique(finserv)
```

### Step 2: Point-in-Polygon Count Process

Next, we will count the number of financial services in each hexagon and include this as a variable in **area_hexagon_grid** table.

```{r}
area_hexagon_grid$`COUNT_FS`<- lengths(
  st_intersects(
    area_hexagon_grid, finserv))
```

The summary statistics reveal that up to 130 financial services locations can be found within a single hexagon, with less than half of the hexagons are devoid of any such locations.

```{r}
summary(area_hexagon_grid$COUNT_FS)
```

Let's visualise where these are located, alongside the top 5% of ridership, to see if we can identify any discernible patterns.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons(col='#C2D3CC', border.alpha = 0.1, alpha = 0.3) +
tm_shape(area_hexagon_grid) +
  tm_fill(
    col="COUNT_FS",
    style="pretty",
    palette="Blues",
    alpha=0.7) +
  tm_borders(alpha = 0.5) +
flowLine %>%  
  filter(MORNING_PEAK >= 1399) %>% 
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.1)+
    tm_layout(legend.text.size = 0.6,
              legend.title.size=0.6)
```

Similarly, the relationship between ridership and the count of financial services is not particularly obvious. High ridership is observed outside the Central Business District area, where financial services are concentrated.

## Leisure & Recreation Centres

**recs** includes information on various leisure and recreation centers, such as playgrounds, parks, and fitness centers. It also contains their respective **point** geometries in the CRS SVY21.

Recreational facilities could be popular for early morning workouts and might be an **attractor** for the morning crowd.

### Step 1: Import shapefile

```{r}
#| code-fold: true
#| code-summary: "Show the code"
recs <- st_read(dsn = "data/geospatial",
                      layer = "Liesure&Recreation") %>%
          st_transform(crs = 3414)
```

Note that there are 1217 locations of leisure and recreational centres in our dataset in total.

The results from the duplicate check show that no duplicates were found

```{r}
finserv %>%
  group_by_all() %>%
  filter(n() > 1) %>%
  ungroup() %>%
  arrange(POI_NAME) %>% 
  kable()
```

### Step 2: Point-in-Polygon Count Process

Next, we will count the number of facilities in each hexagon and include this as a variable in **area_hexagon_grid** table.

```{r}
area_hexagon_grid$`COUNT_RECS`<- lengths(
  st_intersects(
    area_hexagon_grid, recs))
```

The summary statistics indicate that, on average, a single leisure and recreational facility is found within each hexagon, although the highest number recorded in a hexagon is 41.

```{r}
summary(area_hexagon_grid$COUNT_RECS)
```

Let's visualise where these are located, alongside the top 5% of ridership, to see if we can identify any discernible patterns.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons(col='#C2D3CC', border.alpha = 0.1, alpha = 0.3) +
tm_shape(area_hexagon_grid) +
  tm_fill(
    col="COUNT_RECS",
    style="pretty",
    palette="Blues",
    alpha=0.7) +
  tm_borders(alpha = 0.5) +
flowLine %>%  
  filter(MORNING_PEAK >= 1399) %>% 
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.1)+
    tm_layout(legend.text.size = 0.6,
              legend.title.size=0.6)
```

Similarly, the relationship between the number of bus trips and the presence of leisure and recreational centers is not consistently clear across all areas. Locations such as the CBD, Yishun, and Tampines, which have a higher concentration of recreational centers, do indeed experience higher ridership.

## Retail

**retail** includes information on various retail and services stores/outlets, along with their respective **point** geometries in the CRS SVY21.

Retail locations can be significant **attractors** in the morning, particularly as employment destinations for people who work in these retail and service outlets. Some retail services, like coffee shops, breakfast spots, and convenience stores, might attract early morning customers, including commuters heading to work.

### Step 1: Import shapefile

```{r}
#| code-fold: true
#| code-summary: "Show the code"
retail <- st_read(dsn = "data/geospatial",
                  layer = "Retails") %>%
          st_transform(crs = 3414)
```

Note that there are 37635 retail locations in our dataset in total.

Similarly, we remove duplicates to prevent double-counting. This reduces the number of retail services locations from 37,635 to 37,460.

```{r}
retail <- unique(retail)
```

### Step 2: Point-in-Polygon Count Process

Next, we will count the number of retail centres in each hexagon and include this as a variable in **area_hexagon_grid** table.

```{r}
area_hexagon_grid$`COUNT_RETAIL`<- lengths(
  st_intersects(
    area_hexagon_grid, retail))
```

The summary statistics reveal that, on average, 44 retail and service centers can be located within a single hexagon, with the maximum number reaching 1669.

```{r}
summary(area_hexagon_grid$COUNT_RETAIL)
```

Let's visualise where these are located, alongside the top 5% of ridership, to see if we can identify any discernible patterns.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons(col='#C2D3CC', border.alpha = 0.1, alpha = 0.3) +
tm_shape(area_hexagon_grid) +
  tm_fill(
    col="COUNT_RETAIL",
    style="pretty",
    palette="Blues",
    alpha=0.7) +
  tm_borders(alpha = 0.5) +
flowLine %>%  
  filter(MORNING_PEAK >= 1399) %>% 
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.1)+
    tm_layout(legend.text.size = 0.6,
              legend.title.size=0.6)
```

Significant passenger flows are observed in the Town area where the retail count is high. However, high passenger flows are also noted in areas where retail units are not as abundant, such as Woodlands and Jurong.

## Schools

The *schools* data is taken from *School Directory and Information* as of March 2022 from data.gov.

Schools significantly impact local transit patterns, especially during morning and afternoon peak hours. Regular travel routines of students, teachers, and staff create predictable demand, with a high volume of commuters contributing substantially to local transit ridership, making it a critical **attractor**. This effect is particularly pronounced on bus routes serving school areas, where the demand for public transit is heightened around school start and end times.

### Step 1: Geocoding using SLA OneMap API

We first load our data into RStudio using `read_csv()`, which allows us to access data from a CSV file. The dataset contains detailed descriptions of mainly primary and secondary schools, ranging from addresses to contact information.

```{r}
#| eval: false
csv <- read_csv('data/aspatial/Generalinformationofschools.csv')

glimpse(schs)
```

Note that this data is aspatial. In subsequent steps, we aim to geocode using the postal code in the *postal_code* field to obtain the geographic coordinates of each school. The Singapore Land Authority (SLA) supports an online geocoding service called (**OneMap API**)\[https://www.onemap.gov.sg/apidocs/\]. The Search API looks up address data or a 6-digit postal code for an entered value, then returns the latitude, longitude, and x,y coordinates of the searched location.

The following code chunks will perform geocoding using the SLA OneMap API. It uses the input data **csv** and a collection of HTTP call functions from the **httr** package in R to pass individual records to the geocoding server at OneMap.

The codes below aim to extract the field required for geocoding, i.e., *postal_code*, and set the link to the SLA OneMap API.

```{r}
#| eval: false
pacman::p_load(httr)

## Extract field
postcodes <- csv$`postal_code`

## Establish link with SLA OneMap API
url<-"https://www.onemap.gov.sg/api/common/elastic/search"

```

The code below performs the following:

-   Creates two tibble data frames if the geocoding process completes successfully: **found** and **not_found**. **found** contains all records that are geocoded correctly, and **not_found** contains postals that failed to be geocoded.

-   **found** data table will be joined with the initial csv data table using a unique identifier (i.e., POSTAL) common to both data tables. The output data table will then be saved as a CSV file called **found**.

```{r}
#| eval: false
#| 
found<-data.frame()
not_found<-data.frame()

for(postcode in postcodes){
  query<-list('searchVal'=postcode,'returnGeom'='Y','getAddrDetails'='Y','pageNum'='1')
  res<- GET(url,query=query)
  
  if((content(res)$found)!=0){
    found<-rbind(found,data.frame(content(res))[4:13])
  } else{
    not_found = data.frame(postcode)
  }
}
```

Next, the code chunk below will combine both **found** and **not_found** data.frames into a single tibble data.frame called **merged**.

```{r}
#| eval: false

merged = merge(csv, found, by.x = 'postal_code', by.y = 'results.POSTAL', all = TRUE)
```

Then we will write **merged** and **not_found** tibble data.frames into two separate csv files called **schools** and **not_found** respectively.

```{r}
#| eval: false

write.csv(merged, file = "data/aspatial/schools.csv")
write.csv(not_found, file = "data/aspatial/not_found.csv")
```

Note that Zhenghua Secondary School did not manage to be geocoded using the OneMap API. We can fill in the gaps by manually inputting the longitude and latitude information from Google.

Manual Edit in **schools.csv**:

![](images/ZHSS.PNG){width="276"}

Save the file in Excel once this step is completed.

### Step 2: Import csv file

```{r}
schools <- read_csv("data/aspatial/schools.csv") %>%
  rename(latitude = "results.LATITUDE",
         longitude = "results.LONGITUDE")%>%
  select(postal_code, school_name, latitude, longitude)
```

A quick check for duplicates showed that there are 4 duplicates found, we remove these to prevent double-counting.

```{r}
schools %>%
  group_by_all() %>%
  filter(n() > 1) %>%
  ungroup() %>%
  arrange(school_name) %>% 
  kable()
```

Note that the number of schools has dropped from 350 to 346 after running the code chunk below.

```{r}
schools <- unique(schools)
```

### Step 3: Convert into sf tibble data.frame

To transform longitude and latitude into point geometries for further analysis, we can use `st_as_sf` to convert coordinates into spatial geometry objects and `st_transform` to help in assigning to Singapore's coordinate system with EPSG 3414.

```{r}
schools_sf <- st_as_sf(schools, 
                       coords = c("longitude", "latitude"),
                       crs=4326) %>%
  st_transform(crs = 3414)
```

### Step 4: Point-in-Polygon Count Process

Next, we will count the number of schools in each hexagon and include this as a variable in *area_hexagon_grid* table.

```{r}
area_hexagon_grid$`COUNT_SCHOOLS`<- lengths(
  st_intersects(
    area_hexagon_grid, schools_sf))
```

The summary statistics reveal that more than half of the hexagons do not contain any schools, with the maximum number of schools in a single hexagon being 4.

```{r}
summary(area_hexagon_grid$COUNT_SCHOOLS)
```

Let's visualize where these are located, alongside the top 5% of ridership, to see if we can identify any discernible patterns.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons(col='#C2D3CC', border.alpha = 0.1, alpha = 0.3) +
tm_shape(area_hexagon_grid) +
  tm_fill(
    col="COUNT_SCHOOLS",
    style="pretty",
    palette="Blues",
    alpha=0.7) +
  tm_borders(alpha = 0.5) +
flowLine %>%  
  filter(MORNING_PEAK >= 1399) %>% 
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.1)+
    tm_layout(legend.text.size = 0.6,
              legend.title.size=0.6)

```

There seems to be an association between passenger volume and the number of schools. There is a significantly higher number of flow lines representing the top 5% of ridership in areas with higher school counts, as indicated by the darker blue shades on the map.

## HDBs

The *hdb* dataset is a geocoded version of the *HDB Property Information* from data.gov, provided courtesy of Prof. Kam. It encompasses details such as addresses and characteristics of Housing Development Board (HDB) blocks, complete with longitude and latitude coordinates. The number of total_dwelling_units in these blocks serves as an indicative measure of the **propulsive** factor during weekday morning peak periods.

Significantly, HDB blocks house the majority of Singapore's population, making this dataset particularly valuable for urban and transportation planning. The number of dwelling units in a housing area is a robust indicator of population density. To enhance the accuracy of our population estimates, we multiply the count of dwelling units by 3.09, which represents the average household size in Singapore as of 2022. This adjustment allows for a more realistic assessment of population density ((SingStat, 2022))\[https://www.singstat.gov.sg/find-data/search-by-theme/households/households/latest-data\]. Typically, higher density translates to a greater potential for public transport usage, especially buses, during peak commuting times. Consequently, areas with a high concentration of dwelling units, and hence higher estimated population, are likely to see substantial demand for public transportation services during peak hours as residents commute to work, school, or other daily activities.

### Step 1: Import csv file

We first import the csv file using `read_csv()`.

```{r}
hdb <- read_csv("data/aspatial/hdb.csv")

str(hdb)
```

Subsequently, the below code chunk retains relevant columns for further analysis:

```{r}
hdb <- hdb %>%  
  select(c("blk_no", "street", "total_dwelling_units", "lat", "lng"))
  
```

### Step 2: Convert into sf tibble data.frame

To transform longitude and latitude into point geometries for further analysis, we can use `st_as_sf` to convert coordinates into spatial geometry objects and `st_transform` to help in assigning to Singapore's coordinate system with EPSG 3414.

```{r}
hdb_sf <- st_as_sf(hdb, 
                       coords = c("lng", "lat"),
                       crs=4326) %>%
  st_transform(crs = 3414)
```

### Step 3: Counting the Total Dwelling Units / Population per Hexagon

`st_intersects()` performs a spatial join between *area_hexagon_grid* and *hdb_sf* so that we can identify HDBs that are located in hexagons with bus stop.

`apply()` applies a function to each row. Within the function, hdb_sf\$total_dwelling_units\[row\] sums the total_dwelling_units values for all hdb_sf features that intersect with the hexagons. To obtain a more accurate estimation of the actual population in each hexagon, we then multiply this sum by 3.09, which is the average household size in Singapore as of 2022. This multiplication adjusts the total dwelling units to better reflect the population density, providing a more realistic basis for analyzing and planning urban and transportation needs.

```{r}
hdbinhex  <-  st_intersects(area_hexagon_grid, 
                            hdb_sf)

area_hexagon_grid$TOT_HDBPOP <- 
  apply(hdbinhex, 1, function(row) {
    sum(hdb_sf$total_dwelling_units[row], na.rm = TRUE) * 3.09
  })
```

A quick `sum()` of the total population from all the dwelling units suggests that our proxy of 3.3 million is acceptable, as [The Straits Times](https://www.straitstimes.com/singapore/singapore-resident-population-in-hdb-flats-falls-to-304m-with-smaller-households-spread) reported a total of 3.1 million residents living in HDB flats in 2021.

```{r}
sum(area_hexagon_grid$TOT_HDBPOP)
```

Let's visualize the population density across the island, alongside the top 5% of ridership, to see if we can identify any discernible patterns.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons(col='#C2D3CC', border.alpha = 0.1, alpha = 0.3) +
tm_shape(area_hexagon_grid) +
  tm_fill(
    col="TOT_HDBPOP",
    style="pretty",
    palette="Blues",
    alpha=0.7) +
  tm_borders(alpha = 0.5) +
flowLine %>%  
  filter(MORNING_PEAK >= 1399) %>% 
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.1)+
    tm_layout(legend.text.size = 0.6,
              legend.title.size=0.6)

```

There seems to be a correlation between the density of the population and the volume of bus ridership. Areas with denser populations are marked by a greater number of flow lines, as shown by the darker blue shades on the map.
:::

# 7 Combining Attributes with Flow Data

Two `left_joins()` will be performed between *flow_data* and *area_hexagon_grid* using the origin and destination grid IDs.

Before performing the left joins, we first have to convert *grid_id* of *\`*area_hexagon_grid\* into factor data type.

```{r}
area_hexagon_grid$grid_id  <- as.factor(area_hexagon_grid$grid_id)
```

::: panel-tabset
## Preparing Origin Attributes

```{r}
flowdata_ori <- flow_data %>% 
  left_join(area_hexagon_grid,
            by=c('ORIGIN_GRID' = 'grid_id')) %>% 
  rename(ORI_GEOM = geometry,
         ORI_BS = busstops,
         ORI_TRAINEXITS = COUNT_TRAINSTATIONEXIT,
         ORI_BIZ = COUNT_BIZ,
         ORI_FS = COUNT_FS,
         ORI_RECS = COUNT_RECS,
         ORI_RETAIL = COUNT_RETAIL,
         ORI_SCHOOLS = COUNT_SCHOOLS,
         ORI_HDBPOP = TOT_HDBPOP)

glimpse(flowdata_ori)
```

## Preparing Destination Attributes

```{r}
flowdata_final <- flowdata_ori %>% 
  left_join(area_hexagon_grid,
            by=c('DESTIN_GRID' = 'grid_id')) %>% 
  rename(TRIPS = MORNING_PEAK,
        DEST_GEOM = geometry,
        DEST_BS = busstops,
        DEST_TRAINEXITS = COUNT_TRAINSTATIONEXIT,
        DEST_BIZ = COUNT_BIZ,
        DEST_FS = COUNT_FS,
        DEST_RECS = COUNT_RECS,
        DEST_RETAIL = COUNT_RETAIL,
        DEST_SCHOOLS = COUNT_SCHOOLS,
        DEST_HDBPOP = TOT_HDBPOP) %>% 
  select(-c('ORI_GEOM','ORI_BS','DEST_GEOM','DEST_BS'))

glimpse(flowdata_final)
```
:::

# 8 Preparing for Modelling

## 8.1 Visualising Distribution of Variables

::: panel-tabset
## Trips (Dependent Variable)

Firstly, let us plot the distribution of the dependent variable (i.e. TRIPS) by using histogram method by using the code chunk below.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Extract column
dist_Trips <- flowdata_final$TRIPS
# Calculate mean 
dist_Trips_mean <- mean(dist_Trips)

ggplot(
    data = data.frame(dist_Trips),
    aes(x = dist_Trips)
  ) +
  geom_histogram(
    bins = 20, 
    color = "#FFFCF9", 
    fill = "black",
    alpha = .3
  ) +
  # Add line for mean
  geom_vline(
    xintercept = dist_Trips_mean, 
    color = "#595DE5", 
    linetype = "dashed", 
    linewidth = 1
  ) +
  # Add line annotations
  annotate(
    "text", 
    x = 10000, 
    y = 4000,
    label = paste("Mean =", round(dist_Trips_mean, 3)),
    color = "#595DE5",
    size = 3
  ) +
  labs(
    title = "Distribution of Trips",
    x = "Bus Trips",
    y = "Frequency"
  ) 
```

Notice that the distribution is highly right-skewed.

## Independent Variables

The code chunk below is used to create histograms for all origin and destination related variables. Then, `ggarrange()` is used to organised these histogram into a 3 columns by 3 rows of multiple small plots.

**Origin Variables**

```{r}
#| code-fold: true
#| code-summary: "Show the code"

plot_ori_ts <- ggplot(data=flowdata_final, aes(x= `ORI_TRAINEXITS`)) + 
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_ori_biz <- ggplot(data=flowdata_final, aes(x= `ORI_BIZ`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_ori_fs <- ggplot(data=flowdata_final, aes(x= `ORI_FS`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_ori_recs <- ggplot(data=flowdata_final, aes(x= `ORI_RECS`)) + 
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_ori_retail <- ggplot(data=flowdata_final, aes(x= `ORI_RETAIL`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_ori_schools <- ggplot(data=flowdata_final, aes(x= `ORI_SCHOOLS`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_ori_hdbpop <- ggplot(data=flowdata_final, aes(x= `ORI_HDBPOP`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)



ggarrange(plot_ori_ts, plot_ori_biz, plot_ori_fs, 
          plot_ori_recs, plot_ori_retail, plot_ori_schools,
          plot_ori_hdbpop,
          ncol = 3, nrow = 3)
```

**Destination Variables**

```{r}
#| code-fold: true
#| code-summary: "Show the code"

plot_des_ts <- ggplot(data=flowdata_final, aes(x= `DEST_TRAINEXITS`)) + 
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_des_biz <- ggplot(data=flowdata_final, aes(x= `DEST_BIZ`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_des_fs <- ggplot(data=flowdata_final, aes(x= `DEST_FS`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_des_recs <- ggplot(data=flowdata_final, aes(x= `DEST_RECS`)) + 
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_des_retail <- ggplot(data=flowdata_final, aes(x= `DEST_RETAIL`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_des_schools <- ggplot(data=flowdata_final, aes(x= `DEST_SCHOOLS`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_des_hdbpop <- ggplot(data=flowdata_final, aes(x= `DEST_HDBPOP`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)



ggarrange(plot_des_ts, plot_des_biz, plot_des_fs, 
          plot_des_recs, plot_des_retail, plot_des_schools,
          plot_des_hdbpop,
          ncol = 3, nrow = 3)
```

The distribution for the dependent variables are highly right-skewed as well.

## Trips against Distance

Next, let us visualise the relation between the dependent variable and distance, which is one of the key independent variable in our Spatial Interaction Model.

```{r}
#| code-fold: true
#| code-summary: "Show the code"


tripvdist <- ggplot(data = flowdata_final,
       aes(x = dist,
           y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)+
  labs(
    title = "Graph of Trips against Distance Travelled",
    x = "Distance",
    y = "No. of Trips"
  ) +
  theme(plot.title = element_text(size=10),
        axis.title=element_text(size=10))

logtripvslogdist <- ggplot(data = flowdata_final,
       aes(x = log(dist),
           y = log(TRIPS)))+
  geom_point()+
  geom_smooth(method = lm) +
    labs(
    title = "Graph of log(Trips) against log(Distance)",
    x = "log(Distance)",
    y = "log(Trips)"
  ) +
  theme(plot.title = element_text(size=10),
        axis.title=element_text(size=10))

ggarrange(tripvdist, logtripvslogdist, nrow = 1, ncol = 2)
```

The scatter plot on the left using the original values indicates that the relationship between trips and distance does not demonstrate a linear relationship. However, when we plot the scatter plot using the log-transformed version of both variables, the relationship appears more like an inverse linear relationship. This pattern is indicative of distance decay, a concept in spatial analysis where the interaction between locations decreases as the distance between them increases. The log transformation helps in visualizing and quantifying this distance decay effect, where a greater distance is associated with a lower number of trips, reflecting a common trend in spatial interactions and movements.
:::

## 8.2 Poisson Regression

Poisson regression is appropriate for our dataset for two main reasons:

-   Count Dependent Variable: Our dependent variable (TRIPS) is a count (i.e., the number of occurrences of an event). Linear regression, on the other hand, assumes that the dependent variable is continuous and can take any value, including negative numbers, which is not applicable for count data.

-   Predicting Non-Negative Values: Poisson regression naturally ensures that predictions are non-negative, which is essential for count data. Linear regression can predict negative values, which do not make sense for counts.

It is important here that the explanatory variables are **never zero** since Poisson Regression is base on log and log 0 is undefined. In the code chunk below, `summary()` of **Base R** is used to compute the summary statistics of all variables in flowdata_final data frame.

```{r}
summary(flowdata_final)
```

The report above reveals that variables ORI_TRAINEXITS, ORI_BIZ, ORI_FS, ORI_RECS, ORI_RETAIL, ORI_SCHOOLS, ORI_HDBPOP, DEST_TRAINEXITS, DEST_BIZ, DEST_FS, DEST_RECS, DEST_RETAIL, DEST_SCHOOLS, DEST_HDBPOP consist of 0 values.

In view of this, code chunk below will be used to replace zero values to an arbitrary value of 0.99.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Origin Attributes

flowdata_final$ORI_TRAINEXITS <- ifelse(
  flowdata_final$ORI_TRAINEXITS == 0,
  0.99, 
  flowdata_final$ORI_TRAINEXITS)

flowdata_final$ORI_BIZ <- ifelse(
  flowdata_final$ORI_BIZ == 0,
  0.99, 
  flowdata_final$ORI_BIZ)

flowdata_final$ORI_FS <- ifelse(
  flowdata_final$ORI_FS == 0,
  0.99, 
  flowdata_final$ORI_FS)

flowdata_final$ORI_RECS <- ifelse(
  flowdata_final$ORI_RECS == 0,
  0.99, 
  flowdata_final$ORI_RECS)

flowdata_final$ORI_RETAIL <- ifelse(
  flowdata_final$ORI_RETAIL == 0,
  0.99, 
  flowdata_final$ORI_RETAIL)

flowdata_final$ORI_SCHOOLS <- ifelse(
  flowdata_final$ORI_SCHOOLS == 0,
  0.99, 
  flowdata_final$ORI_SCHOOLS)

flowdata_final$ORI_HDBPOP <- ifelse(
  flowdata_final$ORI_HDBPOP == 0,
  0.99, 
  flowdata_final$ORI_HDBPOP)

# Destination Attributes

flowdata_final$DEST_TRAINEXITS <- ifelse(
  flowdata_final$DEST_TRAINEXITS == 0,
  0.99, 
  flowdata_final$DEST_TRAINEXITS)

flowdata_final$DEST_BIZ <- ifelse(
  flowdata_final$DEST_BIZ == 0,
  0.99, 
  flowdata_final$DEST_BIZ)

flowdata_final$DEST_FS <- ifelse(
  flowdata_final$DEST_FS == 0,
  0.99, 
  flowdata_final$DEST_FS)

flowdata_final$DEST_RECS <- ifelse(
  flowdata_final$DEST_RECS == 0,
  0.99, 
  flowdata_final$DEST_RECS)

flowdata_final$DEST_RETAIL <- ifelse(
  flowdata_final$DEST_RETAIL == 0,
  0.99, 
  flowdata_final$DEST_RETAIL)

flowdata_final$DEST_SCHOOLS <- ifelse(
  flowdata_final$DEST_SCHOOLS == 0,
  0.99, 
  flowdata_final$DEST_SCHOOLS)

flowdata_final$DEST_HDBPOP <- ifelse(
  flowdata_final$DEST_HDBPOP == 0,
  0.99, 
  flowdata_final$DEST_HDBPOP)
```

We run `summary()` again to check the results.

```{r}
summary(flowdata_final)
```

## 8.3 Extracting Inter- & Intra-Zonal Flow Data

To calibrate separate Spatial Interaction Models for inter- and intra-zonal flows, we can select the data required using the *Zone_Type* variable created previously.

Next, inter-zonal flow will be selected from flow_data and saved into a new output data.frame called **interzonal_flow** using the code chunk below, and intra-zonal flow will be filtered for and saved in **intrazonal_flow**.

```{r}
interzonal_flow <- flowdata_final %>%
  filter(Zone_Type=='interzone')

intrazonal_flow <- flowdata_final %>%
  filter(Zone_Type=='intrazone')
```

## 8.4 Correlation Analysis

Before building a Poisson regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. Multicollinearity in a regression model can compromise the quality of the model.

The code chunk below uses the [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) package to plot a scatterplot matrix of the relationship between the independent variables in **interzonal_flow** data.frame. AOE order is used, tt orders the variables by using the *angular order of the eigenvectors* method suggested by [Michael Friendly](https://www.datavis.ca/papers/corrgram.pdf).

::: panel-tabset
## Interzonal Flow

```{r}
#| code-fold: true
#| code-summary: "Show the code"

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))

corrplot(cor(interzonal_flow[, c(10:23)]), 
         diag = FALSE, 
         order = "AOE",
         tl.pos = "td", 
         tl.cex = 0.5, # Change size of headers
         number.cex = 0.5, # Change size of coefficients
         method="color", col=col(200),
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, #Text label color and rotation
         type = "upper")
```

From the correlation matrix, it is observed that none of the variable pairs exhibit a correlation greater than 0.8. Consequently, with no issues of multicollinearity present, there is no need to exclude any variables from our analysis.

## Intrazonal Flow

```{r}
#| code-fold: true
#| code-summary: "Show the code"

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))

corrplot(cor(intrazonal_flow[, c(10:23)]), 
         diag = FALSE, 
         order = "AOE",
         tl.pos = "td", 
         tl.cex = 0.5, # Change size of headers
         number.cex = 0.5, # Change size of coefficients
         method="color", col=col(200),
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, #Text label color and rotation
         type = "upper")
```

The following pairs of variables exhibit high collinearity, each with a correlation coefficient of 1

-   HDB population at origin and destination

-   School count at origin and destination

-   Business count at origin and destination

-   Retail count at origin and destination

-   Financial services count at origin and destination

-   Train station exit count at origin and destintation

This high degree of collinearity is expected for intrazonal flows, where the origin and destination are the same. However, caution is advised in subsequent analytical steps to avoid using these highly correlated variables together, as they can lead to issues in statistical models.
:::

# 9 Calibrating Spatial Interaction Models for Interzonal Travels

A spatial interaction model is specifically designed to map and model the interactivity between various factors across distinct locations. This makes it particularly useful for understanding data involving more than one location component, such as our analysis of bus travel from one hexagon to another.

In this section, we will calibrate spatial interaction models for interzonal data using the *glm()* function from the **Base Stats** package in R and determine the statistical significance of the association between the explanatory variables and the response variable, before moving on to evaluate how well the models fit our data.

## 9.1 Unconstrained SIM

::: panel-tabset
## Model

In this unconstrained model, the following variables are used:

-   Origin propulsiveness: ORI_TRAINEXITS, ORI_HDBPOP

-   Destination attractiveness: DEST_TRAINEXITS, DEST_BIZ, DEST_FS, DEST_RECS, DEST_RETAIL, DEST_SCHOOLS

```{r}
uncSIM <- glm(formula = TRIPS ~ 
                log(ORI_TRAINEXITS) +
                #log(ORI_BIZ) +
                #log(ORI_FS) +
                #log(ORI_RECS) +
                #log(ORI_RETAIL) +
                #log(ORI_SCHOOLS) +
                log(ORI_HDBPOP) +
                log(DEST_TRAINEXITS) +
                log(DEST_BIZ) +
                log(DEST_FS) +
                log(DEST_RECS) +
                log(DEST_RETAIL) +
                log(DEST_SCHOOLS) +
                #log(DEST_HDBPOP) +
                log(dist),
              family = poisson(link = "log"),
              data = interzonal_flow,
              na.action = na.exclude) # excludes any NAs in the data

```

## Coefficients & p-Values

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| 
summary(uncSIM)
```

The p-values associated with each predictor variable is \< 0.05, this suggests that all the coefficients used in the model have a statistically significant relationship with the weekday morning peak period bus trips.

From the results, we can also see that the top 2 coefficients with positive relationships are: *number of train station exits at destination* (0.46) and *number of schools at destination* (0.27).

The top 2 coefficients with inverse relationships are: *distance* (-1.45) and *number of leisure / recreational places at destination* (-0.39).

```{r}
#| code-fold: true
#| code-summary: "Show the code"

data.frame(
  Coefficient = sort(uncSIM$coefficients, decreasing = TRUE)

)
```
:::

## 9.2 Origin-Constrained SIM

::: panel-tabset
## Model

For origin-constrained model, only explanatory variables representing the attractiveness at the destinations will be used. ORIGIN_GRID, in the categorial data type, is used to model .

-1 is added in the equation after the distance variable. The -1 serves the purpose of removing the intercept that *glm* will insert into the model by default.

```{r}
orcSIM <- glm(formula = TRIPS ~ 
                ORIGIN_GRID +
                #log(ORI_TRAINEXITS) +
                #log(ORI_BIZ) +
                #log(ORI_FS) +
                #log(ORI_RECS) +
                #log(ORI_RETAIL) +
                #log(ORI_SCHOOLS) +
                #log(ORI_HDBPOP) +
                log(DEST_TRAINEXITS) +
                log(DEST_BIZ) +
                log(DEST_FS) +
                log(DEST_RECS) +
                log(DEST_RETAIL) +
                log(DEST_SCHOOLS) +
                #log(DEST_HDBPOP) +
                log(dist) - 1,
              family = poisson(link = "log"),
              data = interzonal_flow,
              na.action = na.exclude) # excludes any NAs in the data

```

## Coefficients & p-Values

```{r}
#| code-fold: true
#| code-summary: "Show the code"
summary(orcSIM)

```

The p-values associated with each predictor variable is \< 0.05, this suggests that all coefficients are statistically significant in determining weekday morning peak period bus trips.

From the results below, we can also see that the top 2 coefficients with positive relationships are: *number of train station exits at destination* (0.50) and the *number of financial services locations at destination* (0.26).

The top 2 coefficients with inverse relationships are: *distance* (-1.51) and *number of leisure / recreational places at destination* (-0.24).

```{r}
#| code-fold: true
#| code-summary: "Show the code"
data.frame(
  Coefficient = sort(orcSIM$coefficients[815:821]
                     , decreasing = TRUE
  )
)
```
:::

## 9.3 Destination-Constrained SIM

::: panel-tabset
## Model

```{r}
decSIM <- glm(formula = TRIPS ~ 
                DESTIN_GRID +
                log(ORI_TRAINEXITS) +
                #log(ORI_BIZ) +
                #log(ORI_FS) +
                #log(ORI_RECS) +
                #log(ORI_RETAIL) +
                #log(ORI_SCHOOLS) +
                log(ORI_HDBPOP) +
                #log(DEST_TRAINEXITS) +
                #log(DEST_BIZ) +
                #log(DEST_FS) +
                #log(DEST_RECS) +
                #log(DEST_RETAIL) +
                #log(DEST_SCHOOLS) +
                #log(DEST_HDBPOP) +
                log(dist) - 1,
              family = poisson(link = "log"),
              data = interzonal_flow,
              na.action = na.exclude) # excludes any NAs in the data

```

## Coefficients & p-Values

```{r}
#| code-fold: true
#| code-summary: "Show the code"
summary(decSIM)
```

The p-values associated with each predictor variable is \< 0.05, indicating that all coefficients are statistically significant in determining weekday morning peak period bus trips using the destination-constrained SIM.

The results below show that the *number of train station exits at origin* and the *number of population residing in HDBs at the origin* demonstrated positive association with the trips travelled during weekday morning peak periods. In contrast, distance had a negative association with the dependent variable instead.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
data.frame(
  Coefficient = sort(decSIM$coefficients[816:818]
                     , decreasing = TRUE
  )
)
```
:::

## 9.4 Doubly Constrained SIM

::: panel-tabset
## Model

```{r}
dbcSIM <- glm(formula = TRIPS ~ 
                ORIGIN_GRID +
                DESTIN_GRID +
                log(dist),
              family = poisson(link = "log"),
              data = interzonal_flow,
              na.action = na.exclude) # excludes any NAs in the data

```

## Coefficients & p-Values

```{r}
#| code-fold: true
#| code-summary: "Show the code"
summary(dbcSIM)
```

Variable *distance* is statistically significant with p-value \< 0.05 and a coefficient of -1.6

```{r}
#| code-fold: true
#| code-summary: "Show the code"
data.frame(
  Coefficient = sort(dbcSIM$coefficients[1629]
                     , decreasing = TRUE
  )
)
```
:::

# 10 Model Comparison for Interzonal Travels

Suitable statistical measures help evaluate how well the models' parameters give the best fit to data. In this section, we will look at various models that can help us determine which model is 'best'.

## 10.1 R-Squared

Calculating R-Squared helps determine the proportion of variance in the dependent variable that can be explained by the independent variables. In other words, R-Squared shows how well the data fit the regression model (the goodness of fit).

To obtain R-Squared value, we will be using the following helper function:

```{r}
rsq <- function(observed,estimated){
  r <- cor(observed,estimated)
  R2 <- r^2
  R2
}
```

::: panel-tabset
## Unconstrained SIM

An R-squared value of 0.26 means that approximately 26% of the variance in the TRIPS dependent variable is explained by the independent variables in the model.

```{r}
rsq(uncSIM$data$TRIPS, uncSIM$fitted.values)
```

## Origin-Constrained SIM

An R-squared value of 0.41 means that approximately 41% of the variance in the TRIPS dependent variable is explained by the independent variables in the model.

```{r}
rsq(orcSIM$data$TRIPS, orcSIM$fitted.values)
```

## Destination-Constrained SIM

An R-squared value of 0.41 means that approximately 41% of the variance in the TRIPS dependent variable is accounted for by the explanatory variables in the model.

```{r}
rsq(decSIM$data$TRIPS, decSIM$fitted.values)
```

## Doubly Constrained SIM

An R-squared value of 0.58 means that approximately 58% of the variance in the TRIPS dependent variable is explained by the independent variables in the model.

```{r}
rsq(dbcSIM$data$TRIPS, dbcSIM$fitted.values)
```
:::

Table below summarises of the R-Squared values yielded by the 4 different SIMs:

| Model                   | R-Squared Value |
|-------------------------|-----------------|
| Unconstrained           | 0.257           |
| Origin-Constrained      | 0.408           |
| Destination-Constrained | 0.402           |
| Doubly-Cconstrained     | 0.577           |

Doubly-constrained model yielded the highest R-Squared value amongst the 4 models calibrated.

## 10.2 Root Mean Squared Error

[Root Mean Squared Error](https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e) (RMSE) is a useful model performance measure to evaluate how closely a model's predicted values match the actual values. We explore how to utilise the [`compare_performance()`](https://easystats.github.io/performance/reference/compare_performance.html) function from [**performance**](https://easystats.github.io/performance/) package to evaluate this measure.

The code chunk below serves a dual purpose:

-   It first creates a list named **model_list**.

-   Then, it computes the Root Mean Squared Error (RMSE) for all the models in `model_list` using the `compare_performance()` function.

```{r}
model_list <- list(
  Unconstrained = uncSIM,
  Origin_Constrained = orcSIM,
  Desintation_Constrained = decSIM,
  Doubly_Constrained = dbcSIM)

compare_performance(model_list,
                    metrics = "RMSE")
```

Similar to our R-Squared results, the results above reveal that doubly constrained SIM is the best model as it has the smallest RMSE value of 1164.166 amongst the 4 models calibrated.

## 10.3 Visualising Fitted Values

Plotting observed versus fitted values allows us to visually assess how well the model captures the data. If the model is a good fit, the fitted values should closely follow the trend of the observed values. Such visualizations can help in identifying patterns or anomalies that might not be apparent from the model's statistical output alone. In this section, we will visualise the observed values and the fitted values.

The code chunk below performs the followinresidualg functions:

-   It first extracts the fitted values from each model

-   Appends the fitted values into **inter_zonal_flow** data frame

-   Lastly, `rename()` is used to rename the field name

::: panel-tabset
## Unconstrained SIM

```{r}
# Extracts fitted values from model
df <- as.data.frame(uncSIM$fitted.values) %>%
    round(digits = 0)

# Append fitted values to inter_zonal_flow
interzonal_flow  <- interzonal_flow  %>%
    cbind(df) %>%
    rename(uncTRIPS = "uncSIM$fitted.values")
```

## Origin-Constrained SIM

```{r}
# Extracts fitted values from model
df <- as.data.frame(orcSIM$fitted.values) %>%
    round(digits = 0)

# Append fitted values to inter_zonal_flow
interzonal_flow  <- interzonal_flow  %>%
    cbind(df) %>%
    rename(orcTRIPS = "orcSIM$fitted.values")
```

## Destination-Constrained SIM

```{r}
# Extracts fitted values from model
df <- as.data.frame(decSIM$fitted.values) %>%
    round(digits = 0)

# Append fitted values to inter_zonal_flow
interzonal_flow  <- interzonal_flow  %>%
    cbind(df) %>%
    rename(decTRIPS = "decSIM$fitted.values")
```

## Doubly Constrained SIM

```{r}
# Extracts fitted values from model
df <- as.data.frame(dbcSIM$fitted.values) %>%
    round(digits = 0)

# Append fitted values to inter_zonal_flow
interzonal_flow  <- interzonal_flow  %>%
    cbind(df) %>%
    rename(dbcTRIPS = "dbcSIM$fitted.values")
```
:::

Using `ggplot()` and `ggarrange()`, we will visualise the observed against fitted values of the 4 models in a single visual for better comparison.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

library(plotly)
unc_p <- ggplot(data = interzonal_flow,
                aes(x = uncTRIPS,
                    y = TRIPS)) +
  geom_point(shape = 16, size = 0.5) +
  geom_smooth(method = lm) +
  ggtitle("Unconstrained SIM") +
  theme(
plot.title = element_text(size=8, face="bold"),
axis.title.x = element_text(size=8, face="bold"),
axis.title.y = element_text(size=8, face="bold"),
axis.text.x = element_text(size=6),
axis.text.y = element_text(size=6)
)

orc_p <- ggplot(data = interzonal_flow,
                aes(x = orcTRIPS,
                    y = TRIPS)) +
  geom_point(shape = 16, size = 0.5) +
  geom_smooth(method = lm)  +
  ggtitle("Origin-constrained SIM") +
  theme(
plot.title = element_text(size=8, face="bold"),
axis.title.x = element_text(size=8, face="bold"),
axis.title.y = element_text(size=8, face="bold"),
axis.text.x = element_text(size=6),
axis.text.y = element_text(size=6)
)

dec_p <- ggplot(data = interzonal_flow,
                aes(x = decTRIPS,
                    y = TRIPS)) +
  geom_point(shape = 16, size = 0.5) +
  geom_smooth(method = lm) +
  ggtitle("Destination-constrained SIM") +
  theme(
plot.title = element_text(size=8, face="bold"),
axis.title.x = element_text(size=8, face="bold"),
axis.title.y = element_text(size=8, face="bold"),
axis.text.x = element_text(size=6),
axis.text.y = element_text(size=6)
)

dbc_p <- ggplot(data = interzonal_flow,
                aes(x = dbcTRIPS,
                    y = TRIPS)) +
  geom_point(shape = 16, size = 0.5) +
  geom_smooth(method = lm) +
  ggtitle("Doubly-constrained SIM") +
  theme(
plot.title = element_text(size=8, face="bold"),
axis.title.x = element_text(size=8, face="bold"),
axis.title.y = element_text(size=8, face="bold"),
axis.text.x = element_text(size=6),
axis.text.y = element_text(size=6)
)

ggarrange(unc_p, orc_p, dec_p, dbc_p,
          ncol = 2,
          nrow = 2)
```

From the four plots above, it is evident that most data points do not align with the best fit line. The origin-constrained and doubly-constrained models appear to have a comparatively better fit, as more points cluster close to the line of perfect fit, suggesting that these models' predictions are more aligned with the actual data. Conversely, the unconstrained and destination-constrained models display a broader scatter of points, further from the line, indicating a potential deficiency in capturing the underlying trend effectively. The relatively flat line observed for the unconstrained SIM implies limited predictive power, as there is minimal variation in the fitted values across the observed data range, thus failing to capture the data's variability adequately.

Additionally, there are notable outliers on the top middle of the Origin-constrained and Doubly-constrained SIM models where the values are furthest from the best fit line. Such outliers can exert a disproportionate influence on the model, and therefore, their impact warrants careful evaluation.

## 10.3.1 Removing Outliers

Upon further investigation, the outliers correspond to the observations detailed below.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

datatable(interzonal_flow[interzonal_flow$TRIPS %in% c(77433, 77255, 76780), ])
```

To analyse the impact of the outliers on our model fit, we remove the outliers using the `filter()` function and recalibrated the models using `glm()`.

```{r}
interzonal_flow2 <- interzonal_flow %>%
  filter(!TRIPS %in% c(77433, 77255, 76780))
```

```{r}
uncSIM2 <- glm(formula = TRIPS ~ 
                log(ORI_TRAINEXITS) +
                #log(ORI_BIZ) +
                #log(ORI_FS) +
                #log(ORI_RECS) +
                #log(ORI_RETAIL) +
                #log(ORI_SCHOOLS) +
                log(ORI_HDBPOP) +
                log(DEST_TRAINEXITS) +
                log(DEST_BIZ) +
                log(DEST_FS) +
                log(DEST_RECS) +
                log(DEST_RETAIL) +
                log(DEST_SCHOOLS) +
                #log(DEST_HDBPOP) +
                log(dist),
              family = poisson(link = "log"),
              data = interzonal_flow2,
              na.action = na.exclude) # excludes any NAs in the data

orcSIM2 <- glm(formula = TRIPS ~ 
                ORIGIN_GRID +
                #log(ORI_TRAINEXITS) +
                #log(ORI_BIZ) +
                #log(ORI_FS) +
                #log(ORI_RECS) +
                #log(ORI_RETAIL) +
                #log(ORI_SCHOOLS) +
                #log(ORI_HDBPOP) +
                log(DEST_TRAINEXITS) +
                log(DEST_BIZ) +
                log(DEST_FS) +
                log(DEST_RECS) +
                log(DEST_RETAIL) +
                log(DEST_SCHOOLS) +
                #log(DEST_HDBPOP) +
                log(dist) - 1,
              family = poisson(link = "log"),
              data = interzonal_flow2,
              na.action = na.exclude) # excludes any NAs in the data

decSIM2 <- glm(formula = TRIPS ~ 
                DESTIN_GRID +
                log(ORI_TRAINEXITS) +
                #log(ORI_BIZ) +
                #log(ORI_FS) +
                #log(ORI_RECS) +
                #log(ORI_RETAIL) +
                #log(ORI_SCHOOLS) +
                log(ORI_HDBPOP) +
                #log(DEST_TRAINEXITS) +
                #log(DEST_BIZ) +
                #log(DEST_FS) +
                #log(DEST_RECS) +
                #log(DEST_RETAIL) +
                #log(DEST_SCHOOLS) +
                #log(DEST_HDBPOP) +
                log(dist) - 1,
              family = poisson(link = "log"),
              data = interzonal_flow2,
              na.action = na.exclude) # excludes any NAs in the data

dbcSIM2 <- glm(formula = TRIPS ~ 
                ORIGIN_GRID +
                DESTIN_GRID +
                log(dist),
              family = poisson(link = "log"),
              data = interzonal_flow2,
              na.action = na.exclude) # excludes any NAs in the data
```

**R-Squared**

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| eval: false
rsq(uncSIM2$data$TRIPS, uncSIM2$fitted.values)
rsq(orcSIM2$data$TRIPS, orcSIM2$fitted.values)
rsq(decSIM2$data$TRIPS, decSIM2$fitted.values)
rsq(dbcSIM2$data$TRIPS, dbcSIM2$fitted.values)

```

The table below compares the R-Squared values before and after the removal of the outliers.

| Model                   | R-Squared Value | R-Squared Value after removing outliers |
|---------------------|------------------|---------------------------------|
| Unconstrained           | 0.257           | 0.267                                   |
| Origin-Constrained      | 0.408           | 0.410                                   |
| Destination-Constrained | 0.402           | 0.425                                   |
| Doubly-Constrained      | 0.577           | 0.580                                   |

**RMSE**

```{r}
#| code-fold: true
#| code-summary: "Show the code"

model_list2 <- list(
  Unconstrained = uncSIM,
  Unconstrained_wo_Outliers = uncSIM2,
  Origin_Constrained = orcSIM,
  Origin_Constrained_wo_Outliers = orcSIM2,
  Desintation_Constrained = decSIM,
  Desintation_Constrained_wo_Outliers = decSIM2,
  Doubly_Constrained = dbcSIM, 
  Doubly_Constrained_wo_Outliers = dbcSIM2)

compare_performance(model_list2,
                    metrics = "RMSE")
```

We observe a marked improvement in both R-Squared and RMSE values following the removal of outliers, with the **Doubly-Constrained model still yielding the best results**.

## 10.3.2 Deviance Residuals vs Fitted Values

Residuals represent the discrepancies between observed values and the predictions made by our model. Ideally, for a model that fits the data well, the residuals should be small and close to zero. Analysing the residuals of GLMs can yield crucial insights into the performance of the model. Deviance residuals, specifically, measure the degree to which probabilities estimated from our model diverge from the observed frequencies of success. Larger values suggest greater divergence, while smaller values indicate less divergence.

The code chunk below extracts residuals, using [`residuals()`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/residuals), and fitted values, using [`fitted()`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/fitted), from two models. Fitted values are the predictions made by the model. For each observation in the model's dataset, it gives the expected value of the response variable, based on the model's fitted parameters.

```{r}

res_dbcSIM <- residuals(dbcSIM, type="deviance")
res_dbcSIM2 <- residuals(dbcSIM2, type="deviance")

fitted_dbcSIM <- fitted(dbcSIM)
fitted_dbcSIM2 <- fitted(dbcSIM2)
```

By employing [`ggplot()`](https://ggplot2.tidyverse.org/), we can compare the plots of residuals versus fitted values for the Doubly-Constrained model with and without outliers. The axis range for both plots are kept constant to visualise the difference between both plots.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Create data frames for plotting
df_dbcSIM <- data.frame(Fitted = fitted_dbcSIM, Residuals = res_dbcSIM)
df_dbcSIM2 <- data.frame(Fitted = fitted_dbcSIM2, Residuals = res_dbcSIM2)

# Create individual ggplots
p_dbcSIM <- ggplot(df_dbcSIM, aes(x = Fitted, y = Residuals)) +
  geom_point(size = 0.5) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Fitted Values", y = "Deviance Residuals", title = "Doubly Constrained SIM") +
  theme(
    plot.title = element_text(size=8, face="bold"),
    axis.title.x = element_text(size=8, face="bold"),
    axis.title.y = element_text(size=8, face="bold"),
    axis.text.x = element_text(size=6),
    axis.text.y = element_text(size=6)
    ) +
  scale_x_continuous(limits = c(0, 60000)) +
  scale_y_continuous(limits = c(-300, 450))  

p_dbcSIM2 <- ggplot(df_dbcSIM2, aes(x = Fitted, y = Residuals)) +
  geom_point(size = 0.5) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Fitted Values", y = "Deviance Residuals", title = "Doubly Constrained SIM w/o Outliers") +
  theme(
    plot.title = element_text(size=8, face="bold"),
    axis.title.x = element_text(size=8, face="bold"),
    axis.title.y = element_text(size=8, face="bold"),
    axis.text.x = element_text(size=6),
    axis.text.y = element_text(size=6)
    ) +
  scale_x_continuous(limits = c(0, 60000)) +
  scale_y_continuous(limits = c(-300, 450)) 

# Arrange the plots in a 2x2 grid
ggarrange(p_dbcSIM, p_dbcSIM2, ncol = 2, nrow = 1)


```

With both plots, the vertical dispersion at lower fitted values suggests signs of overfitting to varying degrees, and to a lesser extent, underfitting. The removal of outliers leads to a decrease in the spread of residuals, particularly in the range of higher residuals. The concentration of residuals around the zero line seems tighter as well, which suggests an improved model fit.

# 11 Calibrating Spatial Interaction Model for Intrazonal Travels

In previous sections, we identified the Doubly Constrained SIM as the most effective model for predicting interzonal flow. This finding underscores the significance of distance in influencing cross-hexagonal travel. In subsequent sections, we will shift our focus to calibrating the optimal model for intrazonal flows. This will enable us to explore whether variables other than distance can effectively explain intrazonal travels, where distance is less of a constraining factor.

In the analysis of intrazonal flows, where movements occur within the same zone, Origin-Constrained, Destination-Constrained, and Doubly Constrained models become less relevant. These models are typically designed to balance flows between distinct origins and destinations, based on specific capacity or potential constraints. However, such constraints are not applicable within the same zone, where origin and destination are identical. In contrast, Unconstrained models are more suitable for intrazonal flows. They do not impose artificial constraints on interaction flows, allowing for a more direct and meaningful examination of local factors. This approach, with its simplicity and direct focus on area-specific characteristics, makes unconstrained models the optimal choice for understanding movements or behaviors that occur within a single zone, driven more by local dynamics than by the balance of flows between different locations.

## 11.1 Unconstrained SIM

**Model**

Note that compared to Unconstrained model used for interzonal travels, origin propulsiveness variable *ORI_TRAINEXITS* has been removed to prevent multicollinearly with *DEST_TRAINEXITS.*

In modeling intrazonal flows, where the origin and destination grids are the same, *log(dist)* has been excluded from the models. This is because the primary factor influencing intrazonal flows is likely not distance, but other area characteristics such as population density, the presence of businesses, and schools. Therefore, the emphasis should be placed more on these local factors rather than on the distance between points within the same zone.

```{r}
intra_uncSIM <- glm(formula = TRIPS ~ 
                #log(ORI_TRAINEXITS) +
                #log(ORI_BIZ) +
                #log(ORI_FS) +
                #log(ORI_RECS) +
                #log(ORI_RETAIL) +
                #log(ORI_SCHOOLS) +
                log(ORI_HDBPOP) +
                log(DEST_TRAINEXITS) +
                log(DEST_BIZ) +
                log(DEST_FS) +
                log(DEST_RECS) +
                log(DEST_RETAIL) +
                log(DEST_SCHOOLS),
                #log(DEST_HDBPOP) +
                #log(dist),
              family = poisson(link = "log"),
              data = intrazonal_flow,
              na.action = na.exclude) # excludes any NAs in the data
```

**Coefficients & p-Values**

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| 
summary(intra_uncSIM)
```

The p-values associated with each predictor variable is \< 0.05, this suggests that all the coefficients used in the model have a statistically significant relationship with the weekday morning peak period bus trips.

From the results, we can also see that the top 2 coefficients with positive relationships are: *number of financial services at destination* (0.72) and *number of train station exits at destination* (0.380).

The top coefficient with inverse relationships is the *number of leisure / recreational places at destination* (-0.47).

```{r}
#| code-fold: true
#| code-summary: "Show the code"

data.frame(
  Coefficient = sort(intra_uncSIM$coefficients, decreasing = TRUE)

)
```

## 11.2 Model Performance

::: panel-tabset
## R-Squared

An R-squared value of 0.277 means that approximately 27.7% of the variance in the TRIPS dependent variable is explained by the independent variables in the model.

```{r}
rsq(intra_uncSIM$data$TRIPS, intra_uncSIM$fitted.values)
```

## Root Mean Squared Error

Since we only have one model for intrazonal flow, we can use [`rmse()`](https://www.rdocumentation.org/packages/Metrics/versions/0.1.4/topics/rmse) directly to measure how far predicted values are from observed values.

```{r}
rmse(intra_uncSIM)
```

A high RMSE suggests that the model may not be very accurate in its predictions. Let's visualise the fitted values to see if we can identify any outliers causing the unpredcitability.

## Visualising Fitted Values

```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Extracts fitted values from model
df <- as.data.frame(intra_uncSIM$fitted.values) %>%
    round(digits = 0)

# Append fitted values to inter_zonal_flow
intrazonal_flow  <- intrazonal_flow  %>%
    cbind(df) %>%
    rename(uncTRIPS = "intra_uncSIM$fitted.values")
```

```{r}
#| code-fold: true
#| code-summary: "Show the code"

p_intra_uncSIM <-
  ggplot(data = intrazonal_flow,
                aes(x = uncTRIPS,
                    y = TRIPS)) +
  geom_point(shape = 16, size = 0.5) +
  geom_smooth(method = lm) +
  ggtitle("Unconstrained SIM for Intrazonal Flow") +
  theme(
plot.title = element_text(size=8, face="bold"),
axis.title.x = element_text(size=8, face="bold"),
axis.title.y = element_text(size=8, face="bold"),
axis.text.x = element_text(size=6),
axis.text.y = element_text(size=6)
)

ggplotly(p_intra_uncSIM)

```

Notably, the point on the top of the graph where TRIPS = 62718 deviates from the best fit line most.

The outlier point corresponds to the intrazonal travel within the Admiralty/Woodlands area below:

```{r}
#| code-fold: true
#| code-summary: "Show the code"

datatable(intrazonal_flow[intrazonal_flow$TRIPS %in% c(62718), ])
```
:::

## 11.3 Removing Outlier

To analyse the impact of the outlier on our model fit, we remove the outlier using the `filter()` function and recalibrated the models using `glm()`.

```{r}
intrazonal_flow2 <- intrazonal_flow %>%
  filter(!TRIPS %in% c(62718))
```

Note that the total observation count has reduced by 1, from 623 to 622.

```{r}
intra_uncSIM2 <- glm(formula = TRIPS ~ 
                #log(ORI_TRAINEXITS) +
                #log(ORI_BIZ) +
                #log(ORI_FS) +
                #log(ORI_RECS) +
                #log(ORI_RETAIL) +
                #log(ORI_SCHOOLS) +
                log(ORI_HDBPOP) +
                log(DEST_TRAINEXITS) +
                log(DEST_BIZ) +
                log(DEST_FS) +
                log(DEST_RECS) +
                log(DEST_RETAIL) +
                log(DEST_SCHOOLS),
                #log(DEST_HDBPOP) +
                #log(dist),
              family = poisson(link = "log"),
              data = intrazonal_flow2,
              na.action = na.exclude) # excludes any NAs in the data
```

**R-Squared**

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| eval: false
rsq(intra_uncSIM2$data$TRIPS, intra_uncSIM2$fitted.values)

```

The table below compares the R-Squared values before and after the removal of the outlier.

| Model         | R-Squared Value | R-Squared Value after removing outlier |
|---------------|-----------------|----------------------------------------|
| Unconstrained | 0.277           | 0.377                                  |

**RMSE**

```{r}
#| code-fold: true
#| code-summary: "Show the code"

model_list_intra <- list(
  Unconstrained = intra_uncSIM,
  Unconstrained_wo_Outliers = intra_uncSIM2)


compare_performance(model_list_intra,
                    metrics = "RMSE")
```

We observe a marked improvement in both R-Squared and RMSE values following the removal of outliers.

## 11.4 Deviance Residuals vs Fitted Values

Similar to the steps performed for interzonal flows, we use `residuals()` and `fitted()` to obtain the deviance residuals and fitted values respectively.

```{r}

res_intra_uncSIM <- residuals(intra_uncSIM, type="deviance")
res_intra_uncSIM2 <- residuals(intra_uncSIM2, type="deviance")

fitted_intra_uncSIM <- fitted(intra_uncSIM)
fitted_intra_uncSIM2 <- fitted(intra_uncSIM2)
```

By employing [*ggplot()*](https://ggplot2.tidyverse.org/), we can compare the plots of residuals versus fitted values for the Doubly-Constrained model with and without outliers. The axis range for both plots are kept constant to visualise the difference between both plots.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Create data frames for plotting
df_intra_unccSIM <- data.frame(Fitted = fitted_intra_uncSIM, Residuals = res_intra_uncSIM)
df_intra_uncSIM2 <- data.frame(Fitted = fitted_intra_uncSIM2, Residuals = res_intra_uncSIM2)

# Create individual ggplots
dr_intra_uncSIM <- ggplot(df_intra_unccSIM, aes(x = Fitted, y = Residuals)) +
  geom_point(size = 0.5) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Fitted Values", y = "Deviance Residuals", title = "Unconstrained SIM") +
  theme(
    plot.title = element_text(size=8, face="bold"),
    axis.title.x = element_text(size=8, face="bold"),
    axis.title.y = element_text(size=8, face="bold"),
    axis.text.x = element_text(size=6),
    axis.text.y = element_text(size=6)
    ) +
  scale_y_continuous(limits = c(-150, 500)) 

dr_intra_uncSIM2 <- ggplot(df_intra_uncSIM2, aes(x = Fitted, y = Residuals)) +
  geom_point(size = 0.5) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Fitted Values", y = "Deviance Residuals", title = "Unconstrained SIM w/o Outliers") +
  theme(
    plot.title = element_text(size=8, face="bold"),
    axis.title.x = element_text(size=8, face="bold"),
    axis.title.y = element_text(size=8, face="bold"),
    axis.text.x = element_text(size=6),
    axis.text.y = element_text(size=6)
    ) +
  scale_y_continuous(limits = c(-150, 500)) 

# Arrange the plots side by side
ggarrange(dr_intra_uncSIM, dr_intra_uncSIM2, ncol = 2, nrow = 1)

```

In general, most of the residuals for Unconstrained SIM are scattered around the zero line, which is desirable. Before removing the outlier, there was significantly higher deviance residuals, indicating large prediction errors for those observations. In the right plot, removal of the outlier led to a tighter clustering of residuals around the zero line, suggesting that the model fit has improved.

# 12 Conclusion & Future Work

In conclusion, we have calibrated 4 models for interzonal travels and 1 model for intrazonal travels.

For interzonal flows, the Doubly-Constrained model has consistently outperformed other models in terms of predictive accuracy, indicating that distance significantly impacts cross-hexagonal movements. Notably, the removal of outliers, particularly at higher passenger volumes, led to a more robust model, as reflected in improved R-Squared and RMSE scores. The Doubly-Constrained model, in particular, showed the best performance after outlier exclusion.

Regarding intrazonal flows, the Unconstrained model indicates that the number of financial services and the number of train station exits at the destination exert the most substantial positive impacts, with coefficients of 0.72 and 0.38, respectively. Conversely, the number of leisure/recreational places at the destination inversely influences ridership at -0.47. This pattern suggests that, during the weekday morning peak period, most people travel to train stations or financial services by bus.

However, it's notable that the best models for both intra- and inter-zonal flows struggle to make meaningful predictions at the highest trip volumes. For interzonal travel, this includes routes such as Woodlands Checkpoint to Kranji MRT Station, Woodlands Checkpoint to Woodlands Regional Bus Interchange, and Choa Chu Kang North to Yew Tee Station. For intrazonal travel, challenges are observed within This could suggest that there may be other influential factors specific to the outlier areas identified during the weekday mornings peak period.

Thus, a deeper understanding in the following areas could lead to more insightful results:

-   Spatial Interaction Models assumes that each hexagon's observations are independent, without accounting for the attractiveness or the draw of surrounding areas. Enhancing these models with spatial econometric techniques, like incorporating weighted metrics, could provide a better grasp of how neighboring areas exert influence.

-   The current calculation of population density, based solely on the number of HDB dwelling units, is a rough estimate. Incorporating other housing types and demographic factors, such as age groups, could provide a more accurate picture. Additionally, the model does not account for young children who may not need to pay for bus rides and thus might not be captured in the trip data.

-   Investigating the connectivity and frequency of buses at each stop, including the usage of feeder buses within neighborhoods compared to long-distance buses to less frequented areas, could yield further insights.

# Reference

epsg.io (2023). EPSG: 3414 SVY21 / Singapore TM. https://epsg.io/3414

Kam, T. S. (2023). 15 Processing and Visualising Flow Data. *R for Geospatial Data Science and Analytics.* https://r4gdsa.netlify.app/chap15

Kam, T. S. (2023). 16 Calibrating Spatial Interaction Models with R. *R for Geospatial Data Science and Analytics.* https://r4gdsa.netlify.app/chap16

Kam, T. S. (2023). In-class Exercise 4: Preparing Spatial Interaction Modelling Variables. *ISSS624.* https://isss624.netlify.app/in-class_ex/in-class_ex4/in-class_ex4-gds

Kam, T. S. (2023). In-class Exercise 4: Calibrating Spatial Interaction Models with R. *ISSS624.* https://isss624.netlify.app/in-class_ex/in-class_ex4/in-class_ex4-sims

Miller, E. J. (2021). Traffic Analysis Zone Definition: Issues & Guidance. *Travel Modelling Group.* https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf

SingStat. (2022). Statistics on resident households are compiled by the Singapore Department of Statistics. *Households.* https://www.singstat.gov.sg/find-data/search-by-theme/households/households/latest-data
