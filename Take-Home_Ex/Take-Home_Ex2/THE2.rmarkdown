---
title: "Take-home Exercise 2: Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows"
date: "07 December 2023"
date-modified: "last-modified"
editor: visual
toc-depth: 4
execute:
  freeze: auto
  echo: true #all the codes will appear
  eval: true #all the codes will run
  warning: false #dont display if there are any warnings
format: 
  html:
    code-fold: false
    code-overflow: scroll
    code-summary: "Show the code"
    code-line-numbers: true
---


# 1 Overview

Urban mobility, characterized by the daily commute of urban dwellers from homes to workplaces, presents complex challenges for transport operators and urban managers. Traditional approaches to understanding these mobility patterns, such as commuter surveys, are often hindered by high costs, time-consuming processes, and the rapid obsolescence of collected data. However, the digitalisation of city-wide urban infrastructures, including public buses, mass rapid transits, and other utilities, alongside the advent of pervasive computing technologies like GPS and SMART cards, offers a new paradigm in tracking and analyzing urban movement.

## Objectives

::: panel-tabset
## Aim

This assignment is driven by two primary motivations. First, despite the growing availability of open data for public use, there is a noticeable gap in applied research demonstrating how these diverse data sources can be effectively integrated and analyzed to inform policy-making decisions. Second, there is a need to showcase how GDSA can be utilized in practical decision-making scenarios.

The core task of this assignment is to conduct a case study that exhibits the potential value of GDSA. By leveraging publicly available data from multiple sources, the goal is to build spatial interaction models that unravel the factors influencing urban mobility patterns, particularly focusing on public bus transit. This exercise aims to bridge the gap between the abundance of geospatially-referenced data and its practical application, thereby enhancing the return on investment in data collection and management, and ultimately supporting informed policy-making in urban mobility.

## Tasks

The specific tasks of this take-home exercise are as follows:

### Geospatial Data Science

-   Derive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the [traffic analysis zone (TAZ)](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf).

-   With reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating *Passenger Volume by Origin Destination Bus Stops* and *Bus Stop Location* from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html). The O-D matrix must be aggregated at the analytics hexagon level

    | Peak hour period             | Bus tap on time |
    |------------------------------|-----------------|
    | Weekday morning peak         | 6am to 9am      |
    | Weekday afternoon peak       | 5pm to 8pm      |
    | Weekend/holiday morning peak | 11am to 2pm     |
    | Weekend/holiday evening peak | 4pm to 7pm      |

-   Display the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).

-   Describe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).

-   Assemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.

-   Compute a distance matrix by using the analytical hexagon data derived earlier.

### Spatial Interaction Modelling

-   Calibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.

-   Present the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)

-   With reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual).
:::

# 2 Loading Packages

::: panel-tabset
## Packages

The following packages will be used for this exercise:

| Package                                                                                 | Description                                                              |
|---------------------------------------|---------------------------------|
| [**tmap**](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html) | For thematic mapping                                                     |
| [**sf**](https://r-spatial.github.io/sf/) & [**sp**]()                                  | For importing, integrating, processing, and transforming geospatial data |
| [**tidyverse**](https://www.tidyverse.org/)                                             | For non-spatial data wrangling                                           |
| [**knitr**](https://cran.r-project.org/web/packages/knitr/)                             | For dynamic report generation                                            |
| [**scales**](https://scales.r-lib.org/)                                                 | For scaling graphs                                                       |

## Code


```{r}
pacman::p_load(tmap, sf, sp, 
               tidyverse, DT, performance, 
               reshape2, ggpubr, stplanr,
               knitr, scales, corrplot, 
               gtsummary, plotly)
```

:::

# 3 Data Preparation

For the purpose of this assignment, the following data will be used:

|     |            |                                                                                                                                                                                                                                                                                                                                 |                |            |                                                                                                          |
|------------|------------|------------|------------|------------|------------|
|     | **Type**   | **Name**                                                                                                                                                                                                                                                                                                                        | **As of Date** | **Format** | **Source**                                                                                               |
| 1   | Aspatial   | Passenger Volume by Origin Destination Bus Stops                                                                                                                                                                                                                                                                                | Oct 2023       | .csv       | [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en/dynamic-data.html)                        |
| 2   | Aspatial   | School Directory and Information (General information of schools)                                                                                                                                                                                                                                                               | Mar 2022       | .csv       | [Data.gov.sg](https://beta.data.gov.sg/collections/457/datasets/d_688b934f82c1059ed0a6993d2a829089/view) |
| 3   | Aspatial   | HDB Property Information (Geocoded)                                                                                                                                                                                                                                                                                             | Sep 2021       | .csv       | Courtesy of Prof T. S. Kam                                                                               |
| 4   | Geospatial | Bus Stop Location                                                                                                                                                                                                                                                                                                               | Jul 2023       | .shp       | [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en/static-data.html)                         |
| 5   | Geospatial | Train Station Exit Point                                                                                                                                                                                                                                                                                                        | Aug 2023       | .shp       | [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en/static-data.html)                         |
| 6   | Geospatial | Master Plan 2019 Subzone Boundary                                                                                                                                                                                                                                                                                               | 2019           | .shp       | Courtesy of Prof T.S. Kam                                                                                |
| 7   | Geospatial | Business (incl. industrial parks), FinServ, Leisure&Recreation and Retails (Geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets compiled for urban mobility study) |                | .shp       | Courtesy of Prof T.S. Kam                                                                                |

## 3.1 O-D Data

::: panel-tabset
## Importing csv

*Passenger Volume by Origin Destination Bus Stops* dataset for October 2023, downloaded from LTA DataMall by using *read_csv()* or **readr** package.


```{r}
odbus <- read_csv("data/aspatial/origin_destination_bus_202310.csv")
```


## Attributes

*glimpse()* of the **dplyr** package allows us to see all columns and their data type in the data frame.


```{r}
glimpse(odbus)
```


**Observations:**

-   There are 7 variables in the odbus tibble data, they are:
    -   YEAR_MONTH: Month in which data is collected
    -   DAY_TYPE: Weekdays or weekends/holidays
    -   TIME_PER_HOUR: Hour which the passenger trip is based on, in intervals from 0 to 23 hours
    -   PT_TYPE: Type of public transport, i.e. bus
    -   ORIGIN_PT_CODE: Origin bus stop ID
    -   DESTINATION_PT_CODE: Destination bus stop ID
-   TOTAL_TRIPS: Number of trips We also note that values in ORIGIN_PT_CODE and DESTINATON_PT_CODE are in numeric data type. These should be in factor data type for further processing and georeferencing.

*as.factor()* can be used to convert the variables ORIGIN_PT_CODE and DESTINATON_PT_CODE from numeric to categorical data type. We use glimpse() again to check the results.


```{r}
odbus$ORIGIN_PT_CODE <- as.factor(odbus$ORIGIN_PT_CODE)
odbus$DESTINATION_PT_CODE <- as.factor(odbus$DESTINATION_PT_CODE)

glimpse(odbus)
```


Note that both of them are in factor data type now.

## Extracting Study Data

In our study, we would like to analyse the 1 of the peak hour periods identified. We will be analysing the **Weekday Morning** peak periods thereafter. Therefore, we can employ a combination of the following functions to obtain the relevant data:

Summary of the functions used as follow:

-   *filter()*: Retains rows that satisfies our condition (i.e. Weekday Morning peak period)

-   *select()* of **dplyr** package: Retains the desired variables for further analysis.

-   *group_by()* and *summarise()*: Aggregates the total trips at each combination of origin bus stop, destination bus stop, and peak period.


```{r}
WDMpeak <- odbus %>%
  filter(DAY_TYPE=="WEEKDAY" & (TIME_PER_HOUR >= 6 & TIME_PER_HOUR <= 9)) %>% 
  dplyr::select(5:7)  %>% 
  group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %>% 
  summarise(TRIPS=sum(TOTAL_TRIPS))
```


Let's check the output using the *glimpse()* function of dplyr.


```{r}
glimpse(WDMpeak)
```

:::

## 3.2 Geospatial Data

For the purpose of this exercise, three geospatial data will be used. They are:

-   MPSZ-2019: This data provides the sub-zone boundary of URA Master Plan 2019.
-   BusStop: This data provides the location of bus stop as at Jul 2023.
-   Analytical hexagon: Hexagonal grids of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone.

In this section, we import the shapefiles into RStudio using *st_read()* function of sf package. *st_transform()* function of **sf** package is used to transform the projection to coordinate reference system (CRS) 3414, which is the EPSG code for the SVY21 projection used in Singapore.

::: panel-tabset
## Master Plan Subzone


```{r}
mpsz <- st_read(dsn="data/geospatial",                   
                layer="MPSZ-2019")%>%   
  st_transform(crs = 3414)
```


In the code chunk below, *tm_shape()* of **tmap** package is used to define the input data (i.e mpsz) and tm_polygons() is used to draw the planning subzone polygons.


```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")

tm_shape(mpsz) +
  tm_polygons()
```


## Bus Stop


```{r}
busstop <- st_read(dsn = "data/geospatial",
                   layer = "BusStop") %>% 
  st_transform(crs = 3414)

```


Busstop represents sf point objects for 5161 bus stop in Singapore.

To visualise the points of the bus stops, we can use *tm_shape()* of tmap package with each bus stop point displayed as dots. *tmap_mode* allows us to view static maps with `plot` and interactive maps with `view`.


```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(busstop) +
  tm_dots()+
tm_view(set.zoom.limits = c(11,14)) # to fix the map extent, so cannot zoom in too much
```


Before proceeding, let's check if there are any duplicated bus stops in the dataset.


```{r}
bs_dupes <- busstop %>%
  group_by(BUS_STOP_N) %>%
  filter(n() > 1) %>%
  ungroup() %>%
  arrange(BUS_STOP_N)

knitr::kable(bs_dupes)

```


The results displayed 16 pairs of duplicated 'BUS_STOP_N', with each pair showing a different geometry point for the same bus stop number. This could potentially suggest that these are temporary bus stops. In that case, it would be prudent to retain only one of them, as conventionally, only one bus stop is used at a time.


```{r}
busstop <- busstop %>%
  distinct(BUS_STOP_N, 
           .keep_all = TRUE)

```


Notice that the number of bus stops has dropped from 5161 to 5145.

Note from the choropleth map that there are 5 bus stops located outside Singapore, they are bus stops **46239, 46609, 47701, 46211, and 46219**. The code chunk below uses *filter()* to exclude the 5 bus stops outside Singapore.


```{r}
busstop <- busstop %>%   
  filter(!BUS_STOP_N %in% c(46239, 46609, 47701, 46211, 46219))
```


Notice that the number of bus stops has dropped from 5145 to **5140**.

## Analytical Hexagon

A hexagonal grid is used to represent the traffic analysis zones, which helps to model travel demand through capturing the spatial aspects of trip origins and destinations.

**Step 1: Create Hexagonal Grids**

We first create a hexagonal grid layer of 375m (refers to the perpendicular distance between the centre of the hexagon and its edges) with [*st_make_grid*](https://r-spatial.github.io/sf/reference/st_make_grid.html), [*st_sf*](https://r-spatial.github.io/sf/reference/sf.html) to convert the grid into an sf object with the codes below, and *row_number()* to assign an ID to each hexagon.

::: {.callout-note collapse="true"}
## *st_make_grid* Arguments

*st_make_grid* function is used to create a grid over a spatial object. It takes 4 arguments, they are:

-   x: sf object; the input spatial data

-   cellsize: for hexagonal cells the distance between opposite edges in the unit of the crs the spatial data is using. In this case, we take cellsize to be 375m \* 2 = 750m

![](images/hex.PNG){width="276"}

-   what: character; one of: `"polygons"`, `"corners"`, or `"centers"`
-   square: indicates whether you are a square grid (TRUE) or hexagon grid (FALSE)
:::


```{r}
area_hexagon_grid = st_make_grid(busstop, 
                                 cellsize= 750, 
                                 what = "polygons", 
                                 square = FALSE,
                                 crs = 3414) %>% 
  st_sf() %>% 
  mutate(grid_id = row_number())
  
```


**Step 2: Remove grids with no bus stops**

We count the number of bus stops in each grid and retain only the grids with bus stops using the code chunks below.

[*st_intersects*](https://postgis.net/docs/ST_Intersects.html) is used to identify the bus stops falling inside each hexagon, while [*lengths*](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/lengths) returns the number of bus stops inside each hexagon.


```{r}
# Create a column containing the count of bus stops in each grid
area_hexagon_grid$busstops = lengths(st_intersects(area_hexagon_grid, busstop))

# Retain hexagons with bus stops
area_hexagon_grid = filter(area_hexagon_grid, busstops > 0)

```


Notice that 831 hexagons have been created.

**Step 3: Check & Visualise**


```{r}
sum(area_hexagon_grid$busstops, na.rm = TRUE)
```


Note that there are **5140** bus stops, which tallies to the 5140 from the `Busstop` shape file after deducting for the 5 bus stops outside Singapore boundary and the 16 duplicates, suggesting that the hexagons have managed to capture all expected bus stops.

In the bar chart below, it is evident that the distribution of bus stops per hexagon is right-skewed. While one hexagon contains as many as 19 bus stops, the majority have fewer than 10 bus stops.


```{r}
#| code-fold: true
#| code-summary: "Show the code"

ggplot(area_hexagon_grid, 
       aes(x= as.factor(busstops)))+   
  geom_bar()+   
  ggtitle("No. of Bus Stops per Hexagon") +
  geom_text(aes(label = after_stat(count)), 
            stat = "count", 
            vjust = -0.5, 
            colour = "black")+
  labs(x= "No. of Bus Stops", y = "Count")

```


Lastly, using *tm_shape* from **tmap** package, we can quickly visualise the results of the hexagon grids we have created.


```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode ("plot")
tm_shape(area_hexagon_grid)+
  tm_fill(
    col = "busstops",
    palette = "Blues",
    style = "quantile",
    title = "Number of Bus Stops",
    id = "grid_id",
    showNA = FALSE,
    alpha = 0.6,
    popup.format = list(
      grid_id = list(format = "f", digits = 0)
    )
  )+
  tm_borders(col = "grey40", lwd = 0.7)
```

:::

## 3.3 Geospatial Data Wrangling

### 3.3.1 Combining Busstop and Hexagons

Code chunk below populates the grid ID (i.e. grid_id) of `area_hexagon_grid` sf data frame into `busstop` sf data frame using the following functions:

-   `st_intersection()` is used to perform point and polygon overly and the output will be in point sf object.

-   `select()` of **dplyr** package is then use to retain preferred variables from the data frames.

-   `st_stop_geometry()` removes the geometry data to manipulate it like a regular dataframe using `tidyr` and `dplyr` functions


```{r}
bs_wgrids <- st_intersection(busstop, area_hexagon_grid) %>% 
  dplyr::select(BUS_STOP_N,BUS_ROOF_N,LOC_DESC, grid_id) %>% 
  st_drop_geometry
```


Before we proceed, let's perform a duplicates check on `bs_wgrids`.


```{r}
duplicate <- bs_wgrids %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()

duplicate
```


Results showed that there are no duplicated records.

### 3.3.2 Populate Passenger Volume data with Grid IDs

Next, we are going to append the Grid IDs based on origin bus stops from `bs_wgrids` data frame onto `WDMpeak` data frame.


```{r}
od_data <- left_join(WDMpeak , bs_wgrids,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>% 
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_GRID = grid_id,
         ORIGIN_DESC = LOC_DESC,
         DESTIN_BS = DESTINATION_PT_CODE)
```


Next, we will update od_data data frame with the Grid IDs of destination bus stops.


```{r}
od_data <- left_join(od_data , bs_wgrids,
            by = c("DESTIN_BS" = "BUS_STOP_N")) %>% 
           rename(DESTIN_GRID = grid_id,
                  DESTIN_DESC = LOC_DESC)

glimpse(od_data)
```


The code chunk below allows us to check for duplicates to prevent double counting. The results indicate that there are no duplicates found.


```{r}
duplicate2 <- od_data %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()

duplicate2
```


Next, the code chunk below removes rows with missing data using *drop_na()* and aggregates the total passenger trips at each origin-destination grid level with *group_by()* and *summarise()*. *ORIGIN_nBS* and *DESTIN_nBS* counts the number of bus stops, while *ORIGIN_DESC* and *DESTIN_DESC* provides the descriptions of each of the bus stops at origin and destination grids respectively.


```{r}
od_data <- od_data %>%
  drop_na() %>%
  group_by(ORIGIN_GRID, DESTIN_GRID) %>%
  summarise(MORNING_PEAK = sum(TRIPS),
            ORIGIN_nBS = n_distinct(ORIGIN_BS),
            ORIGIN_DESC = str_c(unique(ORIGIN_DESC), collapse = ", "),
            DESTIN_nBS = n_distinct(DESTIN_BS),
            DESTIN_DESC = str_c(unique(DESTIN_DESC), collapse = ", ")) %>%
  ungroup()
```


Our resulting OD Matrix organises the commuter flows for weekday morning peak period in a column-wise format, with `origin_grid` representing the ***from*** and `destin_grid` representing the ***to***. There are a total of 65,559 unique origin grid to destination grid combinations.


```{r}
#| code-fold: true
#| code-summary: "Show the code"

DT::datatable(od_data,
              options = list(pageLength = 5),
              rownames = FALSE)
```


# 4 Visualising Spatial Interaction

Origin-destination flow maps are a popular option to visualise connections between different spatial locations. It reflects the relationships/flows between locations and are created by monitoring movements. In our analysis, we can use OD flows to identify the patterns of bus ridership during weekday mornings.

## 4.1 Removing Intra-Zonal Flows

Intrazonal travels are considered localised and short duration trips within a transportation analysis zone (i.e. within a hexagon). For our analysis, we will be removing them.


```{r}
od_data1 <- od_data[od_data$ORIGIN_GRID!=od_data$DESTIN_GRID,]
```


There are 623 intra-zonal travels noted from the decrease in observations from 65,559 to 64,936.

## 4.2 OD Flow Distribution


```{r}
#| code-fold: true
#| code-summary: "Show the code"

quantile(od_data1$MORNING_PEAK, 
         probs = seq(0, 1.0, by = .1))
```


From the summary statistics above, the minimum number of passenger trips for each combination of origin and destination bus stop is 1. The maximum observed is 77,433 passengers, occurring during the weekday morning peak period. Furthermore, the 90th percentile is 174 passengers. This data suggests a highly right-skewed distribution.

## 4.2 Creating Flow Lines

Desire lines visually represent the connections between originating and destination hexagons using straight lines. The [od2line()](https://docs.ropensci.org/stplanr/reference/od2line.html) function of **stplanr** package is utilized to create these lines. The width of each desire line is proportional to number of passenger trips, i.e. thicker lines would represent higher ridership.


```{r}
# Creating centroids representing desire line start and end points
flowLine <- od2line(flow = od_data1, 
                    zones = area_hexagon_grid,
                    zone_code = "grid_id")
```


Since there are 65,559 different flow lines resulting from combinations of origin to destination hexagons, an excess of intersecting lines can cause visual clutter and obscure analysis. Considering that the 90th percentile is 658, we will focus on inter-zonal flows with the top 10% of ridership.


```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("view")
tmap_options(check.and.fix = TRUE)

#tm_basemap("OneMapSG.Grey") +
tm_basemap("OpenStreetMap") +
tm_shape(mpsz) +
  tm_polygons(alpha = 0) +
  tm_borders(alpha = 0.5)+
tm_shape(area_hexagon_grid) +
  tm_fill(alpha=0.7, 
          palette="RdBu") + 
  tm_borders(alpha = 0.5)+
flowLine %>%  
  filter(MORNING_PEAK >= 659) %>% 
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.3,
           lines.lwd = "all",
           popup.vars = c("No. of Bus Stops at Origin: " = "ORIGIN_nBS",
                         "Descriptions of Bus Stops at Origin: " = "ORIGIN_DESC",
                         "No. of Bus Stops at Destination: " = "DESTIN_nBS",
                         "Descriptions of Bus Stops at Destination: " = "DESTIN_DESC",
                         "No. of Passenger Trips :" = "MORNING_PEAK"))
#tm_view(set.zoom.limits = c(11,14))
```


The map reveals that Yew Tee, Woodlands, and Yishun dominate bus ridership during weekday mornings, with notably wider desire lines. Key routes include travel within Yew Tee, between Woodlands Checkpoint and Woodlands MRT Station, as well as within Woodlands and Yishun. Interestingly, though broad, these desire lines are relatively short, often indicating bus travel to neighboring hexagons. This suggests a higher demand for feeder bus services in these areas during weekday mornings. Areas such as Boon Lay, Bedok, Choa Chu Kang, Clementi, Tampines, Pasir Ris, and Serangoon also display high concentrations and variations of desire lines with neighboring hexagons, indicating higher ridership within these areas.

Furthermore, longer desire lines between the North and East (i.e., Woodlands and Changi) suggest passengers' willingness to travel longer distances to their destinations.

While OD flows provide valuable insights by quickly visualizing travel patterns, it is beneficial to complement them with other forms of analysis, such as spatial interaction models, for a more comprehensive understanding of the factors affecting urban commuting flow.

# 5 Computing Distance Matrix

A distance matrix is a two-dimensional array containing the distances between different locations. In our analysis, we can use a distance matrix to calculate the distance passengers are willing to travel by bus to get to their destinations.

## 5.1 Converting from sf data.table to SpatialPolygonsDataFrame

Firstly, [`as.Spatial()`](https://r-spatial.github.io/sf/reference/coerce-methods.html) will be used to convert *area_hexagon_grid* from sf tibble data frame to SpatialPolygonsDataFrame of sp object as shown in the code chunk below.


```{r}
hexgrid_sp <- as(area_hexagon_grid, "Spatial")
hexgrid_sp
```


## 5.2 Computing Distance Matrix

Next, [`spDists()`](https://www.rdocumentation.org/packages/sp/versions/2.1-1/topics/spDistsN1) of sp package will be used to compute the Euclidean distance between the centroids of the planning subzones. *spDists* returns a full matrix of distances in the metric of the points if longlat=FALSE, or in kilometers if longlat=TRUE. With 831 hexagons, the return results will produce a 831 by 831 matrix of distance between each hexagon.


```{r}
dist <- spDists(hexgrid_sp, 
                longlat = FALSE)

head(dist, n=c(8, 8))
```


The resulting output is a matrix object class.

Note that column headers and row headers are not labeled with the grid IDs, in the next step, we rename the headers for better clarity.


```{r}
grid_id <- area_hexagon_grid$grid_id

colnames(dist) <- paste0(grid_id)
rownames(dist) <- paste0(grid_id)

head(dist, n=c(8, 8))
```


Notice that the column and row names have been updated to the grid IDs.

## 5.3 Pivoting Distance Value by Grid ID

Next, we will pivot the distance matrix into a long table by using the row and column grid IDs using (*melt()*)\[https://www.rdocumentation.org/packages/reshape2/versions/1.4.4/topics/melt\] of the **reshape2** package, as shown in the code chunk below.


```{r}
distPair <- melt(dist) %>%
  rename(dist = value,
         orig = Var1,
         dest = Var2)

head(distPair, 10)
```


Notice that the within zone distance is 0.

## 5.4 Updating Intra-Zonal Distances

In this section, we are going to append a constant value to replace the intra-zonal distance of 0.

First, we will select and find out the minimum value of the distance by using `summary()`.


```{r}
distPair %>%
  filter(dist > 0) %>%
  summary()
```


Next, an arbitrary constant distance value of 100m is added into intra-zones distance


```{r}
distPair$dist <- ifelse(distPair$dist == 0,
                        100, 
                        distPair$dist)
```


The code chunk below will be used to check the result data.frame.


```{r}
# Check the result data.frame.
summary(distPair)
  
```


Lastly, the code chunk below is used to save the dataframe for future use.


```{r}
write_rds(distPair, "data/rds/distPair.rds") 
```


## 5.5 Combining passenger volume data with distance value

Let's convert the origin and destination grid data in `od_data` and `distPair` into factor data type before we combine passenger volume data from `od_data` and distance from `distPair` using *left_join()*.


```{r}
od_data$ORIGIN_GRID  <- as.factor(od_data$ORIGIN_GRID)
od_data$DESTIN_GRID  <- as.factor(od_data$DESTIN_GRID)

distPair$orig  <- as.factor(distPair$orig)
distPair$dest  <- as.factor(distPair$dest)

flow_data <- od_data %>%
  left_join (distPair,
             by = c("ORIGIN_GRID" = "orig",
                    "DESTIN_GRID" = "dest"))

glimpse(flow_data)
```


## 5.6 Distance Distribution


```{r}
#| code-fold: true
#| code-summary: "Show the code"

quantile(flow_data$dist, 
         probs = seq(0, 1.0, by = .1))
```


From the summary statistics above, the minimum number of passenger trips for each combination of origin and destination bus stop is 100m, which is the arbitrary intrazonal travel distance we have set. The maximum observed is 24,784m.


```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Extract column
distWDM_distance <- flow_data$dist
# Calculate mean 
distWDM_distance_mean <- mean(distWDM_distance)

ggplot(
    data = data.frame(distWDM_distance),
    aes(x = distWDM_distance)
  ) +
  geom_histogram(
    bins = 20, 
    color = "#FFFCF9", 
    fill = "black",
    alpha = .3
  ) +
  # Add line for mean
  geom_vline(
    xintercept = distWDM_distance_mean, 
    color = "#595DE5", 
    linetype = "dashed", 
    linewidth = 1
  ) +
  scale_x_continuous(breaks = pretty(distWDM_distance, n = 10))+
  # Add line annotations
  annotate(
    "text", 
    x = 9000, 
    y = 7500,
    label = paste("Mean =", round(distWDM_distance_mean, 3)),
    color = "#595DE5",
    size = 3
  ) +
  labs(
    title = "Weekday Morning Peak",
    x = "Distance of Bus Trips",
    y = "Frequency"
  ) 
```


## 5.6 Visualise Flow Lines

Since the 90th percentile of the distance traveled is 12,346.558m, we will filter the data for distances greater than 12,347m to analyse the top 10% of the longest distances traveled.

The plot below reveals that longer travel distances predominantly occur between North and North-East regions (i.e., Woodlands/Yishun and Punggol), North and East (Woodlands and Tampines), West and South (Choa Chu Kang/Bukit Panjang and the Town area), and North and South (Yishun and the Town area). These patterns may reflect commuting trends or the distribution of residential and commercial areas, suggesting that significant portions of the population undertake considerable daily commutes. Understanding these long-distance travel patterns is crucial for planning efficient public transport services, particularly in enhancing connectivity between these widely separated areas.

The sparseness of flow lines seen in the West-end also ascertains the trends noted in our previous Take-Home Exercise 1, where low-low autocorrelation were noted in that area.


```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Creating centroids representing desire line start and end points
flowLine2 <- od2line(flow = flow_data, 
                    zones = area_hexagon_grid,
                    zone_code = "grid_id")

tmap_mode("view")
tmap_options(check.and.fix = TRUE)

#tm_basemap("OneMapSG.Grey") +
tm_basemap("OpenStreetMap") +
tm_shape(mpsz) +
  tm_polygons(alpha = 0) +
  tm_borders(alpha = 0.5)+
tm_shape(area_hexagon_grid) +
  tm_fill(alpha=0.7, 
          palette="RdBu") + 
  tm_borders(alpha = 0.5)+
flowLine2 %>%  
  filter(dist >= 12347) %>% 
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.3,
           lines.lwd = "all",
           popup.vars = c("No. of Bus Stops at Origin: " = "ORIGIN_nBS",
                         "Descriptions of Bus Stops at Origin: " = "ORIGIN_DESC",
                         "No. of Bus Stops at Destination: " = "DESTIN_nBS",
                         "Descriptions of Bus Stops at Destination: " = "DESTIN_DESC",
                         "No. of Passenger Trips :" = "MORNING_PEAK"))
#tm_view(set.zoom.limits = c(11,14))
```


Let's now move on to explore the potential factors that could attract or propel passengers to travel by bus from one location to another. This exploration will include examining various urban elements such as the proximity to key amenities like schools, shopping centers, and employment hubs. Understanding these elements can provide valuable insights into improving public transportation systems and urban planning strategies.

# 6. Preparing Origin and Destination Attributes

The following information is used to derive propulsive/attractiveness variables:

1.  *Business*, *FinServ*, *Leisure&Recreation* and *Retails* are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets.

2.  Schools: This data set contains directory and general information of schools in Singapore, obtained from data.gov.

3.  HDB: This data set is the geocoded version of *HDB Property Information* data from data.gov. The data set is prepared using September 2021 data.

::: panel-tabset
## Train Station Exits

**trainstationexits** contains the MRT station names and exits along with their respective **point** geometries in CRS SVY21.

Train stations exits reflect the intermodal connections with bus stops. In the context of **attractiveness**, these stations can be seen as destinations that attract passengers, including those who might transit to/from these stations by bus. The data can also indicate the **propulsiveness** aspect -- how these stations act as origins for passengers who leave the MRT stations and then proceed to their final destinations via other modes of transportation like buses.

### Step 1: Import shapefile

*st_read()* function of the sf package enables us to import the file into RStudio.


```{r}
trainstationexits <- st_read(dsn = "data/geospatial",
                   layer = "Train_Station_Exit_layer") %>% 
  st_transform(crs = 3414)
```


Notice there are 565 train station exits in total.

A quick check for duplicates revealed that there are cases where same station names and exit codes have different geometries. In the absence of further information, it is prudent to retain all these details, as they might represent cases where lifts and escalators at the exits are located at different points.


```{r}
trainstationexits %>%
  group_by(stn_name, exit_code) %>%
  filter(n() > 1) %>%
  ungroup() %>%
  arrange(stn_name, exit_code) %>% 
  kable()

```


### Step 2: Point-in-Polygon Count Process

Next, we will count the number of train station exits located inside each hexagon and include this as a variable in `area_hexagon_grid` table.


```{r}
area_hexagon_grid$`COUNT_TRAINSTATIONEXIT`<- lengths(
  st_intersects(
    area_hexagon_grid, trainstationexits))

sum(area_hexagon_grid$COUNT_TRAINSTATIONEXIT)
```


The 5 train station exits not accounted for could be in areas outside hexagons where there are no bus stop.

Summary statistics indicate that a maximum of 13 train station exits are located within a single hexagon, while at least 75% of the hexagons do not contain any exits.


```{r}
summary(area_hexagon_grid$COUNT_TRAINSTATIONEXIT)
```


Let's visualise where these are located:


```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons(alpha = 0) +
  tm_borders(alpha = 0.5)+
tm_shape(area_hexagon_grid) +
  tm_fill(alpha=0.7, 
          palette="RdBu") +
  tm_borders(alpha = 0.5)+
tm_shape(trainstationexits) +
  tm_dots()
```


## Business

**business** contains the details of various businesses from SMEs to bigger groups like Pan Pacific, as well as the respective **point** geometries in CRS SVY21.

Businesses serve as significant **attractors** in a city. They draw people to these locations, primarily for work purposes. The presence and density of businesses in an area can significantly influence the flow of commuters, making it a measure of attractiveness.

### Step 1: Import shapefile


```{r}
#| code-fold: true
#| code-summary: "Show the code"
business <- st_read(dsn = "data/geospatial",
                      layer = "Business") %>%
          st_transform(crs = 3414)
```


Note that there are 6550 business in our dataset in total.

Our duplicate check showed KEPPEL DISTRIPARK is included twice in the dataset. We remove this to prevent double-counting.


```{r}
business %>%
  group_by_all() %>%
  filter(n() > 1) %>%
  ungroup() %>%
  arrange(POI_NAME) %>% 
  kable()
```

```{r}
business <- unique(business)
```


Note that the number of records for `business` is has decreased from 6550 to 6549.

### Step 2: Point-in-Polygon Count Process

Next, we will count the number of business in each hexagon and include this as a variable in `area_hexagon_grid` table.


```{r}
area_hexagon_grid$`COUNT_BIZ`<- lengths(
  st_intersects(
    area_hexagon_grid, business))
```


Summary statistics below show that less than half of the hexagons contain no businesses, while in contrast, a single hexagon houses as many as 97 businesses.


```{r}
summary(area_hexagon_grid$COUNT_BIZ)
```


Let's visualise where these are located are using the code chunk below:


```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons(alpha = 0) +
  tm_borders(alpha = 0.5)+
tm_shape(area_hexagon_grid) +
  tm_fill(alpha=0.7, 
          palette="RdBu") +
  tm_borders(alpha = 0.5)+
tm_shape(trainstationexits) +
  tm_dots()
```


## Financial Centres

**finserv** contains the details of various financial centres, as well as the respective **point** geometries in CRS SVY21.

Financial service locations often represent significant employment centers, especially in urban and commercial areas. Many people travel to these locations for work, making them important **attractors** during morning peak hours. In addition, financial services typically adhere to standard office hours, which aligns well with the morning peak period of bus ridership, as a large proportion of employees would be traveling to work during this time.

### Step 1: Import shapefile


```{r}
#| code-fold: true
#| code-summary: "Show the code"
finserv <- st_read(dsn = "data/geospatial",
                      layer = "FinServ") %>%
          st_transform(crs = 3414)
```


Note that there are 3320 locations of financial services in our dataset in total.

Similarly, we remove duplicates to prevent double-counting. This reduces the number of financial services locations from 3320 to 3058.


```{r}
finserv <- unique(finserv)
```


### Step 2: Point-in-Polygon Count Process

Next, we will count the number of financial services in each hexagon and include this as a variable in `area_hexagon_grid` table.


```{r}
area_hexagon_grid$`COUNT_FS`<- lengths(
  st_intersects(
    area_hexagon_grid, finserv))
```


The summary statistics reveal that up to 130 financial services locations can be found within a single hexagon, with less than half of the hexagons are devoid of any such locations.


```{r}
summary(area_hexagon_grid$COUNT_FS)
```


Let's visualise where these are located are using the code chunk below:


```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons(alpha = 0) +
  tm_borders(alpha = 0.5)+
tm_shape(area_hexagon_grid) +
  tm_fill(alpha=0.7, 
          palette="RdBu") +
  tm_borders(alpha = 0.5)+
tm_shape(finserv) +
  tm_dots()
```


## Leisure & Recreation Centres

**recs** includes information on various leisure and recreation centers, such as playgrounds, parks, and fitness centers. It also contains their respective **point** geometries in the CRS SVY21.

Recreational facilities could be popular for early morning workouts and might be an **attractor** for the morning crowd.

### Step 1: Import shapefile


```{r}
#| code-fold: true
#| code-summary: "Show the code"
recs <- st_read(dsn = "data/geospatial",
                      layer = "Liesure&Recreation") %>%
          st_transform(crs = 3414)
```


Note that there are 1217 locations of leisure and recreational centres in our dataset in total.

The results from the duplicate check show that no duplicates were found


```{r}
finserv %>%
  group_by_all() %>%
  filter(n() > 1) %>%
  ungroup() %>%
  arrange(POI_NAME) %>% 
  kable()
```


### Step 2: Point-in-Polygon Count Process

Next, we will count the number of facilities in each hexagon and include this as a variable in `area_hexagon_grid` table.


```{r}
area_hexagon_grid$`COUNT_RECS`<- lengths(
  st_intersects(
    area_hexagon_grid, recs))
```


The summary statistics indicate that, on average, a single leisure and recreational facility is found within each hexagon, although the highest number recorded in a hexagon is 41.


```{r}
summary(area_hexagon_grid$COUNT_RECS)
```


Let's visualise where these are located are using the code chunk below:


```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons(alpha = 0) +
  tm_borders(alpha = 0.5)+
tm_shape(area_hexagon_grid) +
  tm_fill(alpha=0.7, 
          palette="RdBu") +
  tm_borders(alpha = 0.5)+
tm_shape(recs) +
  tm_dots()
```


## Retail

**retail** includes information on various retail and services stores/outlets, along with their respective **point** geometries in the CRS SVY21.

Retail locations can be significant **attractors** in the morning, particularly as employment destinations for people who work in these retail and service outlets. Some retail services, like coffee shops, breakfast spots, and convenience stores, might attract early morning customers, including commuters heading to work.

### Step 1: Import shapefile


```{r}
#| code-fold: true
#| code-summary: "Show the code"
retail <- st_read(dsn = "data/geospatial",
                  layer = "Retails") %>%
          st_transform(crs = 3414)
```


Note that there are 37635 retail locations in our dataset in total.

Similarly, we remove duplicates to prevent double-counting. This reduces the number of retail services locations from 37,635 to 37,460.


```{r}
retail <- unique(retail)
```


### Step 2: Point-in-Polygon Count Process

Next, we will count the number of retail centres in each hexagon and include this as a variable in `area_hexagon_grid` table.


```{r}
area_hexagon_grid$`COUNT_RETAIL`<- lengths(
  st_intersects(
    area_hexagon_grid, retail))
```


The summary statistics reveal that, on average, 44 retail and service centers can be located within a single hexagon, with the maximum number reaching 1669.


```{r}
summary(area_hexagon_grid$COUNT_RETAIL)
```


Let's visualise where these are located are using the code chunk below:


```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons(alpha = 0) +
  tm_borders(alpha = 0.5)+
tm_shape(area_hexagon_grid) +
  tm_fill(alpha=0.7, 
          palette="RdBu") +
  tm_borders(alpha = 0.5)+
tm_shape(retail) +
  tm_dots()
```


## Schools

The *schools* data is a geocoded version of the *School Directory and Information* from data.gov. This information was prepared with [OneMap API](https://www.onemap.gov.sg/apidocs/) using the March 2022 data. For detailed steps, please visit [Prof Kam's ISSS624](https://isss624.netlify.app/in-class_ex/in-class_ex4/in-class_ex4-gds).

Schools significantly impact local transit patterns, especially during morning and afternoon peak hours. Regular travel routines of students, teachers, and staff create predictable demand, with a high volume of commuters contributing substantially to local transit ridership, making it a critical **attractor**. This effect is particularly pronounced on bus routes serving school areas, where the demand for public transit is heightened around school start and end times.

### Step 1: Import csv file


```{r}
schools <- read_csv("data/aspatial/schools.csv") %>%
  rename(latitude = "results.LATITUDE",
         longitude = "results.LONGITUDE")%>%
  select(postal_code, school_name, latitude, longitude)
```


A quick check for duplicates showed that there are 4 duplicates found, we remove these to prevent double-counting.


```{r}
schools %>%
  group_by_all() %>%
  filter(n() > 1) %>%
  ungroup() %>%
  arrange(school_name) %>% 
  kable()
```


Note that the number of schools has dropped from 350 to 346.


```{r}
schools <- unique(schools)
```


### Step 2: Convert into sf tibble data.frame

To transform longitude and latitude into point geometries for further analysis, we can use *st_as_sf* to convert coordinates into spatial geometry objects and *st_transform* to help in assigning to Singapore's coordinate system with EPSG 3414.


```{r}
schools_sf <- st_as_sf(schools, 
                       coords = c("longitude", "latitude"),
                       crs=4326) %>%
  st_transform(crs = 3414)
```


### Step 3: Point-in-Polygon Count Process

Next, we will count the number of schools in each hexagon and include this as a variable in `area_hexagon_grid` table.


```{r}
area_hexagon_grid$`COUNT_SCHOOLS`<- lengths(
  st_intersects(
    area_hexagon_grid, schools_sf))
```


The summary statistics reveal that more than half of the hexagons do not contain any schools, with the maximum number of schools in a single hexagon being 4. We see from the map plotted above that two of such hexagons are located at Choa Chu Kang and Marine Parade.


```{r}
summary(area_hexagon_grid$COUNT_SCHOOLS)
```


Let's visualise where these are located:


```{r}
#| code-fold: true
#| code-summary: "Show the code"

tmap_mode("view")
#tm_shape(mpsz) +
#  tm_polygons(alpha = 0) +
#  tm_borders(alpha = 0.5)+
tm_shape(area_hexagon_grid) +
  tm_fill(alpha=0.7, 
          palette="RdBu") +
  tm_borders(alpha = 0.5)+
tm_shape(schools_sf) +
  tm_dots()
```


## HDBs

The *hdb* dataset is a geocoded version of the *HDB Property Information* from data.gov, provided courtesy of Prof. Kam. It encompasses details such as addresses and characteristics of Housing Development Board (HDB) blocks, complete with longitude and latitude coordinates. The number of total_dwelling_units in these blocks serves as an indicative measure of the **propulsive** factor during weekday morning peak periods.

Significantly, HDB blocks house the majority of Singapore's population, making this dataset particularly valuable for urban and transportation planning. The number of dwelling units in a housing area is a robust indicator of population density. To enhance the accuracy of our population estimates, we multiply the count of dwelling units by 3.09, which represents the average household size in Singapore as of 2022. This adjustment allows for a more realistic assessment of population density ((SingStat, 2022))\[https://www.singstat.gov.sg/find-data/search-by-theme/households/households/latest-data\]. Typically, higher density translates to a greater potential for public transport usage, especially buses, during peak commuting times. Consequently, areas with a high concentration of dwelling units, and hence higher estimated population, are likely to see substantial demand for public transportation services during peak hours as residents commute to work, school, or other daily activities.

### Step 1: Import csv file


```{r}
hdb <- read_csv("data/aspatial/hdb.csv")

str(hdb)
```


The below code chunk retains relevant columns for further analysis:


```{r}
hdb <- hdb %>%  
  select(c("blk_no", "street", "total_dwelling_units", "lat", "lng"))
  
```


### Step 2: Convert into sf tibble data.frame

To transform longitude and latitude into point geometries for further analysis, we can use *st_as_sf* to convert coordinates into spatial geometry objects and *st_transform* to help in assigning to Singapore's coordinate system with EPSG 3414.


```{r}
hdb_sf <- st_as_sf(hdb, 
                       coords = c("lng", "lat"),
                       crs=4326) %>%
  st_transform(crs = 3414)
```


### Step 3: Counting the Total Dwelling Units / Population per Hexagon

*st_intersects()* performs a spatial join between `area_hexagon_grid` and `hdb_sf` so that we can identify HDBs that are located in hexagons with bus stop.

*apply()* applies a function to each row. Within the function, hdb_sf\$total_dwelling_units\[row\] sums the total_dwelling_units values for all hdb_sf features that intersect with the hexagons. To obtain a more accurate estimation of the actual population in each hexagon, we then multiply this sum by 3.09, the average household size in Singapore as of 2022. This multiplication adjusts the total dwelling units to better reflect the population density, providing a more realistic basis for analyzing and planning urban and transportation needs.


```{r}
hdbinhex  <-  st_intersects(area_hexagon_grid, 
                            hdb_sf)

area_hexagon_grid$TOT_HDBPOP <- 
  apply(hdbinhex, 1, function(row) {
    sum(hdb_sf$total_dwelling_units[row], na.rm = TRUE) * 3.09
  })
```


A quick sum() of the total population from all the dwelling units suggests that our proxy of 3.3 million is acceptable, as [The Straits Times](https://www.straitstimes.com/singapore/singapore-resident-population-in-hdb-flats-falls-to-304m-with-smaller-households-spread) reported a total of 3.1 million residents living in HDB flats in 2021.


```{r}
sum(area_hexagon_grid$TOT_HDBPOP)
```

:::

# 7. Combining Attributes with Flow Data

Two *left_joins()* will be performed between `flow_data` and `area_hexagon_grid` using the origin and destination grid IDs.

To perform the left join, we first have to convert *grid_id* of `area_hexagon_grid` into factor data type.


```{r}
area_hexagon_grid$grid_id  <- as.factor(area_hexagon_grid$grid_id)
```


::: panel-tabset
## Preparing Origin Attributes


```{r}
flowdata_ori <- flow_data %>% 
  left_join(area_hexagon_grid,
            by=c('ORIGIN_GRID' = 'grid_id')) %>% 
  rename(ORI_GEOM = geometry,
         ORI_BS = busstops,
         ORI_TRAINEXITS = COUNT_TRAINSTATIONEXIT,
         ORI_BIZ = COUNT_BIZ,
         ORI_FS = COUNT_FS,
         ORI_RECS = COUNT_RECS,
         ORI_RETAIL = COUNT_RETAIL,
         ORI_SCHOOLS = COUNT_SCHOOLS,
         ORI_HDBPOP = TOT_HDBPOP)

glimpse(flowdata_ori)
```


## Preparing Destination Attributes


```{r}
flowdata_final <- flowdata_ori %>% 
  left_join(area_hexagon_grid,
            by=c('DESTIN_GRID' = 'grid_id')) %>% 
  rename(TRIPS = MORNING_PEAK,
        DEST_GEOM = geometry,
        DEST_BS = busstops,
        DEST_TRAINEXITS = COUNT_TRAINSTATIONEXIT,
        DEST_BIZ = COUNT_BIZ,
        DEST_FS = COUNT_FS,
        DEST_RECS = COUNT_RECS,
        DEST_RETAIL = COUNT_RETAIL,
        DEST_SCHOOLS = COUNT_SCHOOLS,
        DEST_HDBPOP = TOT_HDBPOP) %>% 
 select(c(1:3,4,5,10:17,6:7,19:26,8)) %>% 
  select(-c('ORI_BS','DEST_BS'))

glimpse(flowdata_final)
```

:::

# 8 Preparing for Modelling

## 8.1 Visualising Distribution of Variables

::: panel-tabset
## Trips (Dependent Variable)

Firstly, let us plot the distribution of the dependent variable (i.e. TRIPS) by using histogram method by using the code chunk below.


```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Extract column
dist_Trips <- flowdata_final$TRIPS
# Calculate mean 
dist_Trips_mean <- mean(dist_Trips)

ggplot(
    data = data.frame(dist_Trips),
    aes(x = dist_Trips)
  ) +
  geom_histogram(
    bins = 20, 
    color = "#FFFCF9", 
    fill = "black",
    alpha = .3
  ) +
  # Add line for mean
  geom_vline(
    xintercept = dist_Trips_mean, 
    color = "#595DE5", 
    linetype = "dashed", 
    linewidth = 1
  ) +
  # Add line annotations
  annotate(
    "text", 
    x = 10000, 
    y = 4000,
    label = paste("Mean =", round(dist_Trips_mean, 3)),
    color = "#595DE5",
    size = 3
  ) +
  labs(
    title = "Distribution of Trips",
    x = "Bus Trips",
    y = "Frequency"
  ) 
```


Notice that the distribution is highly right-skewed.

## Independent Variables

The code chunk below is used to create histograms for all origin and destination related variables. Then, *ggarrange()* is used to organised these histogram into a 3 columns by 3 rows of multiple small plots.

**Origin Variables**


```{r}
#| code-fold: true
#| code-summary: "Show the code"

plot_ori_ts <- ggplot(data=flowdata_final, aes(x= `ORI_TRAINEXITS`)) + 
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_ori_biz <- ggplot(data=flowdata_final, aes(x= `ORI_BIZ`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_ori_fs <- ggplot(data=flowdata_final, aes(x= `ORI_FS`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_ori_recs <- ggplot(data=flowdata_final, aes(x= `ORI_RECS`)) + 
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_ori_retail <- ggplot(data=flowdata_final, aes(x= `ORI_RETAIL`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_ori_schools <- ggplot(data=flowdata_final, aes(x= `ORI_SCHOOLS`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_ori_hdbpop <- ggplot(data=flowdata_final, aes(x= `ORI_HDBPOP`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)



ggarrange(plot_ori_ts, plot_ori_biz, plot_ori_fs, 
          plot_ori_recs, plot_ori_retail, plot_ori_schools,
          plot_ori_hdbpop,
          ncol = 3, nrow = 3)
```


**Destination Variables**


```{r}
#| code-fold: true
#| code-summary: "Show the code"

plot_des_ts <- ggplot(data=flowdata_final, aes(x= `DEST_TRAINEXITS`)) + 
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_des_biz <- ggplot(data=flowdata_final, aes(x= `DEST_BIZ`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_des_fs <- ggplot(data=flowdata_final, aes(x= `DEST_FS`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_des_recs <- ggplot(data=flowdata_final, aes(x= `DEST_RECS`)) + 
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_des_retail <- ggplot(data=flowdata_final, aes(x= `DEST_RETAIL`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_des_schools <- ggplot(data=flowdata_final, aes(x= `DEST_SCHOOLS`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)

plot_des_hdbpop <- ggplot(data=flowdata_final, aes(x= `DEST_HDBPOP`)) +
  geom_histogram(bins = 20, color = "#FFFCF9", fill = "black", alpha = .3)



ggarrange(plot_des_ts, plot_des_biz, plot_des_fs, 
          plot_des_recs, plot_des_retail, plot_des_schools,
          plot_des_hdbpop,
          ncol = 3, nrow = 3)
```


The distribution for the dependent variables are highly right-skewed as well.

## Trips against Distance

Next, let us visualise the relation between the dependent variable and distance, which is one of the key independent variable in our Spatial Interaction Model.


```{r}
#| code-fold: true
#| code-summary: "Show the code"


tripvdist <- ggplot(data = flowdata_final,
       aes(x = dist,
           y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)+
  labs(
    title = "Graph of Trips against Distance Travelled",
    x = "Distance",
    y = "No. of Trips"
  ) +
  theme(plot.title = element_text(size=10),
        axis.title=element_text(size=10))

logtripvslogdist <- ggplot(data = flowdata_final,
       aes(x = log(dist),
           y = log(TRIPS)))+
  geom_point()+
  geom_smooth(method = lm) +
    labs(
    title = "Graph of log(Trips) against log(Distance)",
    x = "log(Distance)",
    y = "log(Trips)"
  ) +
  theme(plot.title = element_text(size=10),
        axis.title=element_text(size=10))

ggarrange(tripvdist, logtripvslogdist, nrow = 1, ncol = 2)
```


The scatter plot on the left using the original values indicates that the relationship between trips and distance does not demonstrate a linear relationship. However, when we plot the scatter plot using the log-transformed version of both variables, the relationship appears more like an inverse linear relationship. This pattern is indicative of distance decay, a concept in spatial analysis where the interaction between locations decreases as the distance between them increases. The log transformation helps in visualizing and quantifying this distance decay effect, where a greater distance is associated with a lower number of trips, reflecting a common trend in spatial interactions and movements.
:::

## 8.2 Poisson Regression

Poisson regression is appropriate for our dataset for two main reasons:

-   Count Dependent Variable: Our dependent variable (TRIPS) is a count (i.e., the number of occurrences of an event). Linear regression, on the other hand, assumes that the dependent variable is continuous and can take any value, including negative numbers, which is not applicable for count data.

-   Predicting Non-Negative Values: Poisson regression naturally ensures that predictions are non-negative, which is essential for count data. Linear regression can predict negative values, which do not make sense for counts.

It is important here that the explanatory variables are **never zero** since Poisson Regression is base on log and log 0 is undefined. In the code chunk below, *summary()* of **Base R** is used to compute the summary statistics of all variables in flowdata_final data frame.


```{r}
summary(flowdata_final)
```


The report above reveals that variables ORI_TRAINEXITS, ORI_BIZ, ORI_FS, ORI_RECS, ORI_RETAIL, ORI_SCHOOLS, ORI_HDBPOP, DEST_TRAINEXITS, DEST_BIZ, DEST_FS, DEST_RECS, DEST_RETAIL, DEST_SCHOOLS, DEST_HDBPOP consist of 0 values.

In view of this, code chunk below will be used to replace zero values to an arbitrary value of 0.99.


```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Origin Attributes

flowdata_final$ORI_TRAINEXITS <- ifelse(
  flowdata_final$ORI_TRAINEXITS == 0,
  0.99, 
  flowdata_final$ORI_TRAINEXITS)

flowdata_final$ORI_BIZ <- ifelse(
  flowdata_final$ORI_BIZ == 0,
  0.99, 
  flowdata_final$ORI_BIZ)

flowdata_final$ORI_FS <- ifelse(
  flowdata_final$ORI_FS == 0,
  0.99, 
  flowdata_final$ORI_FS)

flowdata_final$ORI_RECS <- ifelse(
  flowdata_final$ORI_RECS == 0,
  0.99, 
  flowdata_final$ORI_RECS)

flowdata_final$ORI_RETAIL <- ifelse(
  flowdata_final$ORI_RETAIL == 0,
  0.99, 
  flowdata_final$ORI_RETAIL)

flowdata_final$ORI_SCHOOLS <- ifelse(
  flowdata_final$ORI_SCHOOLS == 0,
  0.99, 
  flowdata_final$ORI_SCHOOLS)

flowdata_final$ORI_HDBPOP <- ifelse(
  flowdata_final$ORI_HDBPOP == 0,
  0.99, 
  flowdata_final$ORI_HDBPOP)

# Destination Attributes

flowdata_final$DEST_TRAINEXITS <- ifelse(
  flowdata_final$DEST_TRAINEXITS == 0,
  0.99, 
  flowdata_final$DEST_TRAINEXITS)

flowdata_final$DEST_BIZ <- ifelse(
  flowdata_final$DEST_BIZ == 0,
  0.99, 
  flowdata_final$DEST_BIZ)

flowdata_final$DEST_FS <- ifelse(
  flowdata_final$DEST_FS == 0,
  0.99, 
  flowdata_final$DEST_FS)

flowdata_final$DEST_RECS <- ifelse(
  flowdata_final$DEST_RECS == 0,
  0.99, 
  flowdata_final$DEST_RECS)

flowdata_final$DEST_RETAIL <- ifelse(
  flowdata_final$DEST_RETAIL == 0,
  0.99, 
  flowdata_final$DEST_RETAIL)

flowdata_final$DEST_SCHOOLS <- ifelse(
  flowdata_final$DEST_SCHOOLS == 0,
  0.99, 
  flowdata_final$DEST_SCHOOLS)

flowdata_final$DEST_HDBPOP <- ifelse(
  flowdata_final$DEST_HDBPOP == 0,
  0.99, 
  flowdata_final$DEST_HDBPOP)
```


We run *summary()* again to check the results.


```{r}
summary(flowdata_final)
```


## 8.3 Extracting Inter-Zonal Flow Data

In general, we will calibrate separate Spatial Interaction Models for inter- and intra-zonal flows. In our analysis, we will focus our attention on inter-zonal flows. Therefore, it is necessary to exclude intra-zonal flows from the flow_data.

First, a new column called FlowNoIntra will be created by using the code chunk below.


```{r}
flowdata_final$FlowNoIntra <- ifelse(
  flowdata_final$ORIGIN_GRID == flowdata_final$DESTIN_GRID, 
  0, 
  flowdata_final$TRIPS)
```


In *FlowNoIntra* field, all intra-zonal flow will be given a value of 0, else the original flow values will be retained.

Next, inter-zonal flow will be selected from flow_data and saved into a new output data.frame called *inter_zonal_flow* using the code chunk below.


```{r}
interzonal_flow <- flowdata_final %>%
  filter(FlowNoIntra > 0)
```


There are 64,936 interzonal flows in our dataset.

## 8.4 Correlation Analysis

Before building a Poisson regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. Multicollinearity in a regression model can compromise the quality of the model.

The code chunk below uses the [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) package to plot a scatterplot matrix of the relationship between the independent variables in *interzonal_flow* data.frame. AOE order is used, tt orders the variables by using the *angular order of the eigenvectors* method suggested by [Michael Friendly](https://www.datavis.ca/papers/corrgram.pdf).


```{r}
#| code-fold: true
#| code-summary: "Show the code"

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))

corrplot(cor(interzonal_flow[, c(6:12, 15:21)]), 
         diag = FALSE, 
         order = "AOE",
         tl.pos = "td", 
         tl.cex = 0.5, # Change size of headers
         number.cex = 0.5, # Change size of coefficients
         method="color", col=col(200),
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, #Text label color and rotation
         type = "upper")
```


From the scatterplot matrix, it is observed that none of the variable pairs exhibit a correlation greater than 0.8. Consequently, with no issues of multicollinearity present, there is no need to exclude any variables from our analysis.

# 9 Calibrating Spatial Interaction Models

A spatial interaction model is specifically designed to map and model the interactivity between various factors across distinct locations. This makes it particularly useful for understanding data involving more than one location component, such as our analysis of bus travel from one hexagon to another.

In this section, we will calibrate spatial interaction models using the *glm()* function from the **Base Stats** package in R and determine the statistical significance of the association between the explanatory variables and the response variable, before moving on to evaluate how well the models fit our data.

## 9.1 Unconstrained SIM

::: panel-tabset
## Model

In this unconstrained model, the following variables are used:

-   Origin propulsiveness: ORI_TRAINEXITS, ORI_HDBPOP

-   Destination attractiveness: DEST_TRAINEXITS, DEST_BIZ, DEST_FS, DEST_RECS, DEST_RETAIL, DEST_SCHOOLS, DEST_HDBPOP


```{r}
uncSIM <- glm(formula = TRIPS ~ 
                log(ORI_TRAINEXITS) +
                #log(ORI_BIZ) +
                #log(ORI_FS) +
                #log(ORI_RECS) +
                #log(ORI_RETAIL) +
                #log(ORI_SCHOOLS) +
                log(ORI_HDBPOP) +
                log(DEST_TRAINEXITS) +
                log(DEST_BIZ) +
                log(DEST_FS) +
                log(DEST_RECS) +
                log(DEST_RETAIL) +
                log(DEST_SCHOOLS) +
                log(DEST_HDBPOP) +
                log(dist),
              family = poisson(link = "log"),
              data = interzonal_flow,
              na.action = na.exclude) # excludes any NAs in the data

```


## Coefficients & p-Values


```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| 
summary(uncSIM)
```


The p-values associated with each predictor variable is \< 0.05, this suggests that all the coefficients used in the model have a statistically significant relationship with the weekday morning peak period bus trips.

From the results, we can also see that the top 2 coefficients with positive relationships are: *number of train station exits at destination* (0.46) and *number of schools at destination* (0.27).

The top 2 coefficients with inverse relationships are: *distance* (-1.45) and *number of leisure / recreational places at destination* (-0.39).


```{r}
#| code-fold: true
#| code-summary: "Show the code"

data.frame(
  Coefficient = sort(uncSIM$coefficients, decreasing = TRUE)

)
```

:::

## 9.2 Origin-Constrained SIM

::: panel-tabset
## Model

For origin-constrained model, only explanatory variables representing the attractiveness at the destinations will be used. ORIGIN_GRID, in the categorial data type, is used to model .

-1 is added in the equation after the distance variable. The -1 serves the purpose of removing the intercept that *glm* will insert into the model by default.


```{r}
orcSIM <- glm(formula = TRIPS ~ 
                ORIGIN_GRID +
                #log(ORI_TRAINEXITS) +
                #log(ORI_BIZ) +
                #log(ORI_FS) +
                #log(ORI_RECS) +
                #log(ORI_RETAIL) +
                #log(ORI_SCHOOLS) +
                #log(ORI_HDBPOP) +
                log(DEST_TRAINEXITS) +
                log(DEST_BIZ) +
                log(DEST_FS) +
                log(DEST_RECS) +
                log(DEST_RETAIL) +
                log(DEST_SCHOOLS) +
                #log(DEST_HDBPOP) +
                log(dist) - 1,
              family = poisson(link = "log"),
              data = interzonal_flow,
              na.action = na.exclude) # excludes any NAs in the data

```


## Coefficients & p-Values


```{r}
#| code-fold: true
#| code-summary: "Show the code"
summary(orcSIM)

```


The p-values associated with each predictor variable is \< 0.05, this suggests that all coefficients are statistically significant in determining weekday morning peak period bus trips.

From the results below, we can also see that the top 2 coefficients with positive relationships are: *number of train station exits at destination* (0.50) and the *number of financial services locations at destination* (0.26).

The top 2 coefficients with inverse relationships are: *distance* (-1.51) and *number of leisure / recreational places at destination* (-0.24).


```{r}
#| code-fold: true
#| code-summary: "Show the code"
data.frame(
  Coefficient = sort(orcSIM$coefficients[815:821]
                     , decreasing = TRUE
  )
)
```

:::

## 9.3 Destination-Constrained SIM

::: panel-tabset
## Model


```{r}
decSIM <- glm(formula = TRIPS ~ 
                DESTIN_GRID +
                log(ORI_TRAINEXITS) +
                #log(ORI_BIZ) +
                #log(ORI_FS) +
                #log(ORI_RECS) +
                #log(ORI_RETAIL) +
                #log(ORI_SCHOOLS) +
                log(ORI_HDBPOP) +
                #log(DEST_TRAINEXITS) +
                #log(DEST_BIZ) +
                #log(DEST_FS) +
                #log(DEST_RECS) +
                #log(DEST_RETAIL) +
                #log(DEST_SCHOOLS) +
                #log(DEST_HDBPOP) +
                log(dist) - 1,
              family = poisson(link = "log"),
              data = interzonal_flow,
              na.action = na.exclude) # excludes any NAs in the data

```


## Coefficients & p-Values


```{r}
#| code-fold: true
#| code-summary: "Show the code"
summary(decSIM)
```


The p-values associated with each predictor variable is \< 0.05, indicating that all coefficients are statistically significant in determining weekday morning peak period bus trips using the destination-constrained SIM.

The results below show that the *number of train station exits at origin* and the *number of population residing in HDBs at the origin* demonstrated positive association with the trips travelled during weekday morning peak periods. In contrast, distance had a negative association with the dependent variable instead.


```{r}
#| code-fold: true
#| code-summary: "Show the code"
data.frame(
  Coefficient = sort(decSIM$coefficients[816:818]
                     , decreasing = TRUE
  )
)
```

:::

## 9.4 Doubly Constrained SIM

::: panel-tabset
## Model


```{r}
dbcSIM <- glm(formula = TRIPS ~ 
                ORIGIN_GRID +
                DESTIN_GRID +
                log(dist),
              family = poisson(link = "log"),
              data = interzonal_flow,
              na.action = na.exclude) # excludes any NAs in the data

```


## Coefficients & p-Values


```{r}
#| code-fold: true
#| code-summary: "Show the code"
summary(dbcSIM)
```


Variable *distance* is statistically significant with p-value \< 0.05 and a coefficient of -1.6


```{r}
#| code-fold: true
#| code-summary: "Show the code"
data.frame(
  Coefficient = sort(dbcSIM$coefficients[1629]
                     , decreasing = TRUE
  )
)
```

:::

# 10 Model Comparison

Suitable statistical measures help evaluate how well the models' parameters give the best fit to data. In this section, we will look at various models that can help us determine which model is 'best'.

## 10.1 R-Squared

Calculating R-Squared helps determine the proportion of variance in the dependent variable that can be explained by the independent variables. In other words, R-Squared shows how well the data fit the regression model (the goodness of fit).

To obtain R-Squared value, we will be using the following helper function:


```{r}
rsq <- function(observed,estimated){
  r <- cor(observed,estimated)
  R2 <- r^2
  R2
}
```


::: panel-tabset
## Unconstrained SIM

An R-squared value of 0.26 means that approximately 26% of the variance in the TRIPS dependent variable is explained by the independent variables in the model.


```{r}
rsq(uncSIM$data$TRIPS, uncSIM$fitted.values)
```


## Origin-Constrained SIM

An R-squared value of 0.41 means that approximately 41% of the variance in the TRIPS dependent variable is explained by the independent variables in the model.


```{r}
rsq(orcSIM$data$TRIPS, orcSIM$fitted.values)
```


## Destination-Constrained SIM

An R-squared value of 0.41 means that approximately 41% of the variance in the TRIPS dependent variable is accounted for by the explanatory variables in the model.


```{r}
rsq(decSIM$data$TRIPS, decSIM$fitted.values)
```


## Doubly Constrained SIM

An R-squared value of 0.58 means that approximately 58% of the variance in the TRIPS dependent variable is explained by the independent variables in the model.


```{r}
rsq(dbcSIM$data$TRIPS, dbcSIM$fitted.values)
```

:::

Table below summarises of the R-Squared values yielded by the 4 different SIMs:

| Model                   | R-Squared Value |
|-------------------------|-----------------|
| Unconstrained           | 0.257           |
| Origin-Constrained      | 0.408           |
| Destination-Constrained | 0.402           |
| Doubly-Cconstrained     | 0.577           |

Doubly-constrained model yielded the highest R-Squared value amongst the 4 models calibrated.

## 10.2 Root Mean Squared Error

[Root Mean Squared Error](https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e) (RMSE) is a useful model performance measure to evaluate how closely a model's predicted values match the actual values. We explore how to utilise the [`compare_performance()`](https://easystats.github.io/performance/reference/compare_performance.html) function from [**performance**](https://easystats.github.io/performance/) package to evaluate this measure.

The code chunk below serves a dual purpose:

-   It first creates a list named `model_list`.

-   Then, it computes the Root Mean Squared Error (RMSE) for all the models in `model_list` using the *compare_performance()* function.


```{r}
model_list <- list(
  Unconstrained = uncSIM,
  Origin_Constrained = orcSIM,
  Desintation_Constrained = decSIM,
  Doubly_Constrained = dbcSIM)

compare_performance(model_list,
                    metrics = "RMSE")
```


Similar to our R-Sqaured results, the results above reveal that doubly constrained SIM is the best model as it has the smallest RMSE value of 1164.166 amongst the 4 models calibrated.

## 10.3 Visualising Fitted Values

Plotting observed versus fitted values allows us to visually assess how well the model captures the data. If the model is a good fit, the fitted values should closely follow the trend of the observed values. Such visualizations can help in identifying patterns or anomalies that might not be apparent from the model's statistical output alone. In this section, we will visualise the observed values and the fitted values.

The code chunk below performs the following functions:

-   It first extracts the fitted values from each model

-   Appends the fitted values into `inter_zonal_flow` data frame

-   Lastly, *rename()* is used to rename the field name

::: panel-tabset
## Unconstrained SIM


```{r}
# Extracts fitted values from model
df <- as.data.frame(uncSIM$fitted.values) %>%
    round(digits = 0)

# Append fitted values to inter_zonal_flow
interzonal_flow  <- interzonal_flow  %>%
    cbind(df) %>%
    rename(uncTRIPS = "uncSIM$fitted.values")
```


## Origin-Constrained SIM


```{r}
# Extracts fitted values from model
df <- as.data.frame(orcSIM$fitted.values) %>%
    round(digits = 0)

# Append fitted values to inter_zonal_flow
interzonal_flow  <- interzonal_flow  %>%
    cbind(df) %>%
    rename(orcTRIPS = "orcSIM$fitted.values")
```


## Destination-Constrained SIM


```{r}
# Extracts fitted values from model
df <- as.data.frame(decSIM$fitted.values) %>%
    round(digits = 0)

# Append fitted values to inter_zonal_flow
interzonal_flow  <- interzonal_flow  %>%
    cbind(df) %>%
    rename(decTRIPS = "decSIM$fitted.values")
```


## Doubly Constrained SIM


```{r}
# Extracts fitted values from model
df <- as.data.frame(dbcSIM$fitted.values) %>%
    round(digits = 0)

# Append fitted values to inter_zonal_flow
interzonal_flow  <- interzonal_flow  %>%
    cbind(df) %>%
    rename(dbcTRIPS = "dbcSIM$fitted.values")
```

:::

Using *ggplot()* and *ggarrange()*, we will visualise the observed against fitted values of the 4 models in a single visual for better comparison.


```{r}
#| code-fold: true
#| code-summary: "Show the code"

library(plotly)
unc_p <- ggplot(data = interzonal_flow,
                aes(x = uncTRIPS,
                    y = TRIPS)) +
  geom_point(shape = 16, size = 0.5) +
  geom_smooth(method = lm) +
  ggtitle("Unconstrained SIM") +
  theme(
plot.title = element_text(size=8, face="bold"),
axis.title.x = element_text(size=8, face="bold"),
axis.title.y = element_text(size=8, face="bold"),
axis.text.x = element_text(size=6),
axis.text.y = element_text(size=6)
)

orc_p <- ggplot(data = interzonal_flow,
                aes(x = orcTRIPS,
                    y = TRIPS)) +
  geom_point(shape = 16, size = 0.5) +
  geom_smooth(method = lm)  +
  ggtitle("Origin-constrained SIM") +
  theme(
plot.title = element_text(size=8, face="bold"),
axis.title.x = element_text(size=8, face="bold"),
axis.title.y = element_text(size=8, face="bold"),
axis.text.x = element_text(size=6),
axis.text.y = element_text(size=6)
)

dec_p <- ggplot(data = interzonal_flow,
                aes(x = decTRIPS,
                    y = TRIPS)) +
  geom_point(shape = 16, size = 0.5) +
  geom_smooth(method = lm) +
  ggtitle("Destination-constrained SIM") +
  theme(
plot.title = element_text(size=8, face="bold"),
axis.title.x = element_text(size=8, face="bold"),
axis.title.y = element_text(size=8, face="bold"),
axis.text.x = element_text(size=6),
axis.text.y = element_text(size=6)
)

dbc_p <- ggplot(data = interzonal_flow,
                aes(x = dbcTRIPS,
                    y = TRIPS)) +
  geom_point(shape = 16, size = 0.5) +
  geom_smooth(method = lm) +
  ggtitle("Doubly-constrained SIM") +
  theme(
plot.title = element_text(size=8, face="bold"),
axis.title.x = element_text(size=8, face="bold"),
axis.title.y = element_text(size=8, face="bold"),
axis.text.x = element_text(size=6),
axis.text.y = element_text(size=6)
)

ggarrange(unc_p, orc_p, dec_p, dbc_p,
          ncol = 2,
          nrow = 2)
```


From the four plots above, it is evident that most data points do not align with the best fit line. The origin-constrained and doubly-constrained models appear to have a comparatively better fit, as more points cluster close to the line of perfect fit, suggesting that these models' predictions are more aligned with the actual data. Conversely, the unconstrained and destination-constrained models display a broader scatter of points, further from the line, indicating a potential deficiency in capturing the underlying trend effectively. The relatively flat line observed for the unconstrained SIM implies limited predictive power, as there is minimal variation in the fitted values across the observed data range, thus failing to capture the data's variability adequately.

Additionally, there are notable outliers on the top middle of the Origin-constrained and Doubly-constrained SIM models where the values are furthest from the best fit line. Such outliers can exert a disproportionate influence on the model, and therefore, their impact warrants careful evaluation.

## 10.3.1 Removing Outliers

Upon further investigation, the outliers correspond to the observations detailed below.


```{r}
#| code-fold: true
#| code-summary: "Show the code"

interzonal_flow[interzonal_flow$TRIPS %in% c(77433, 77255, 76780), ]

```


To analyze the impact of the outliers on our model fit, we removed the outlier using the **`filter()`** function and recalibrated the models using **glm()**.


```{r}
interzonal_flow2 <- interzonal_flow %>%
  filter(!TRIPS %in% c(77433, 77255, 76780))
```

```{r}
uncSIM2 <- glm(formula = TRIPS ~ 
                log(ORI_TRAINEXITS) +
                #log(ORI_BIZ) +
                #log(ORI_FS) +
                #log(ORI_RECS) +
                #log(ORI_RETAIL) +
                #log(ORI_SCHOOLS) +
                log(ORI_HDBPOP) +
                log(DEST_TRAINEXITS) +
                log(DEST_BIZ) +
                log(DEST_FS) +
                log(DEST_RECS) +
                log(DEST_RETAIL) +
                log(DEST_SCHOOLS) +
                log(DEST_HDBPOP) +
                log(dist),
              family = poisson(link = "log"),
              data = interzonal_flow2,
              na.action = na.exclude) # excludes any NAs in the data

orcSIM2 <- glm(formula = TRIPS ~ 
                ORIGIN_GRID +
                #log(ORI_TRAINEXITS) +
                #log(ORI_BIZ) +
                #log(ORI_FS) +
                #log(ORI_RECS) +
                #log(ORI_RETAIL) +
                #log(ORI_SCHOOLS) +
                #log(ORI_HDBPOP) +
                log(DEST_TRAINEXITS) +
                log(DEST_BIZ) +
                log(DEST_FS) +
                log(DEST_RECS) +
                log(DEST_RETAIL) +
                log(DEST_SCHOOLS) +
                #log(DEST_HDBPOP) +
                log(dist) - 1,
              family = poisson(link = "log"),
              data = interzonal_flow2,
              na.action = na.exclude) # excludes any NAs in the data

decSIM2 <- glm(formula = TRIPS ~ 
                DESTIN_GRID +
                log(ORI_TRAINEXITS) +
                #log(ORI_BIZ) +
                #log(ORI_FS) +
                #log(ORI_RECS) +
                #log(ORI_RETAIL) +
                #log(ORI_SCHOOLS) +
                log(ORI_HDBPOP) +
                #log(DEST_TRAINEXITS) +
                #log(DEST_BIZ) +
                #log(DEST_FS) +
                #log(DEST_RECS) +
                #log(DEST_RETAIL) +
                #log(DEST_SCHOOLS) +
                #log(DEST_HDBPOP) +
                log(dist) - 1,
              family = poisson(link = "log"),
              data = interzonal_flow2,
              na.action = na.exclude) # excludes any NAs in the data

dbcSIM2 <- glm(formula = TRIPS ~ 
                ORIGIN_GRID +
                DESTIN_GRID +
                log(dist),
              family = poisson(link = "log"),
              data = interzonal_flow2,
              na.action = na.exclude) # excludes any NAs in the data
```



**R-Squared**

```{r}
#| code-fold: true
#| code-summary: "Show the code"

rsq(uncSIM2$data$TRIPS, uncSIM2$fitted.values)
rsq(orcSIM2$data$TRIPS, orcSIM2$fitted.values)
rsq(decSIM2$data$TRIPS, decSIM2$fitted.values)
rsq(dbcSIM2$data$TRIPS, dbcSIM2$fitted.values)

```


The table below compares the R-Squared values before and after the removal of the outliers.

| Model                   | R-Squared Value | R-Squared Value after removing outliers |
|-------------------------|-----------------|---------------------------------------- |
| Unconstrained           | 0.257           | 0.267                                   |
| Origin-Constrained      | 0.408           | 0.410                                   |
| Destination-Constrained | 0.402           | 0.425                                   |
| Doubly-Constrained      | 0.577           | 0.580                                   |

**RMSE**


```{r}
#| code-fold: true
#| code-summary: "Show the code"

model_list2 <- list(
  Unconstrained = uncSIM,
  Unconstrained_wo_Outliers = uncSIM2,
  Origin_Constrained = orcSIM,
  Origin_Constrained_wo_Outliers = orcSIM2,
  Desintation_Constrained = decSIM,
  Desintation_Constrained_wo_Outliers = decSIM2,
  Doubly_Constrained = dbcSIM, 
  Doubly_Constrained_wo_Outliers = dbcSIM2)

compare_performance(model_list2,
                    metrics = "RMSE")
```


We observe a marked improvement in both R-Squared and RMSE values following the removal of outliers, with the **Doubly-Constrained model still yielding the best results**.

## 10.3.2 Deviance Residuals vs Fitted Values

Residuals represent the discrepancies between observed values and the predictions made by our model. Ideally, for a model that fits the data well, the residuals should be small and close to zero. Analysing the residuals of GLMs can yield crucial insights into the performance of the model. Deviance residuals, specifically, measure the degree to which probabilities estimated from our model diverge from the observed frequencies of success. Larger values suggest greater divergence, while smaller values indicate less divergence.


```{r}
#res_orcSIM <- residuals(orcSIM, type="deviance")
#res_orcSIM2 <- residuals(orcSIM2, type="deviance")
res_dbcSIM <- residuals(dbcSIM, type="deviance")
res_dbcSIM2 <- residuals(dbcSIM2, type="deviance")

#fitted_orcSIM <- fitted(orcSIM)
#fitted_orcSIM2 <- fitted(orcSIM2)

fitted_dbcSIM <- fitted(dbcSIM)
fitted_dbcSIM2 <- fitted(dbcSIM2)
```


By employing [*ggplot()*](https://ggplot2.tidyverse.org/), we can compare the plots of residuals versus fitted values for the Doubly-Constrained model with and without outliers. The axis range for both plots are kept constant to visualise the difference between both plots.


```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Create data frames for plotting
#df_orcSIM <- data.frame(Fitted = fitted_orcSIM, Residuals = res_orcSIM)
#df_orcSIM2 <- data.frame(Fitted = fitted_orcSIM2, Residuals = res_orcSIM2)

df_dbcSIM <- data.frame(Fitted = fitted_dbcSIM, Residuals = res_dbcSIM)
df_dbcSIM2 <- data.frame(Fitted = fitted_dbcSIM2, Residuals = res_dbcSIM2)

# Create individual ggplot objects
#p_orcSIM <- ggplot(df_orcSIM, aes(x = Fitted, y = Residuals)) +
#  geom_point(size = 0.5) +
#  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
#  labs(x = "Fitted Values", y = "Deviance Residuals", title = "Origin-Constrained SIM") +
#  theme(
#    plot.title = element_text(size=8, face="bold"),
#    axis.title.x = element_text(size=8, face="bold"),
#    axis.title.y = element_text(size=8, face="bold"),
#    axis.text.x = element_text(size=6),
#    axis.text.y = element_text(size=6)
#)

#p_orcSIM2 <- ggplot(df_orcSIM2, aes(x = Fitted, y = Residuals)) +
#  geom_point(size = 0.5) +
#  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
#  labs(x = "Fitted Values", y = "Deviance Residuals", title = "Origin-Constrained SIM w/o Outlier") +
#  theme(
#    plot.title = element_text(size=8, face="bold"),
#    axis.title.x = element_text(size=8, face="bold"),
#    axis.title.y = element_text(size=8, face="bold"),
#    axis.text.x = element_text(size=6),
#    axis.text.y = element_text(size=6)
#)

p_dbcSIM <- ggplot(df_dbcSIM, aes(x = Fitted, y = Residuals)) +
  geom_point(size = 0.5) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Fitted Values", y = "Deviance Residuals", title = "Doubly Constrained SIM") +
  theme(
    plot.title = element_text(size=8, face="bold"),
    axis.title.x = element_text(size=8, face="bold"),
    axis.title.y = element_text(size=8, face="bold"),
    axis.text.x = element_text(size=6),
    axis.text.y = element_text(size=6)
    ) +
  scale_x_continuous(limits = c(0, 60000)) +
  scale_y_continuous(limits = c(-300, 450))  

p_dbcSIM2 <- ggplot(df_dbcSIM2, aes(x = Fitted, y = Residuals)) +
  geom_point(size = 0.5) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Fitted Values", y = "Deviance Residuals", title = "Doubly Constrained SIM w/o Outliers") +
  theme(
    plot.title = element_text(size=8, face="bold"),
    axis.title.x = element_text(size=8, face="bold"),
    axis.title.y = element_text(size=8, face="bold"),
    axis.text.x = element_text(size=6),
    axis.text.y = element_text(size=6)
    ) +
  scale_x_continuous(limits = c(0, 60000)) +
  scale_y_continuous(limits = c(-300, 450)) 

# Arrange the plots in a 2x2 grid
ggarrange(p_dbcSIM, p_dbcSIM2, ncol = 2, nrow = 1)


```


With both plots, the vertical dispersion at lower fitted values suggests signs of overfitting to varying degrees, and to a lesser extent, underfitting. The removal of outliers leads to a decrease in the spread of residuals, particularly in the range of higher residuals. The concentration of residuals around the zero line seems tighter as well, which suggests an improved model fit.

# 12 Conclusion & Future Work

The Doubly-Constrained model has consistently outperformed the other models in terms of predictive accuracy. Notably, the removal of outliers resulted in a more robust model, as indicated by enhanced R-Squared and RMSE scores. Specifically, the Doubly-Constrained model demonstrated the best performance after the outliers were excluded. 


```{r}
#| code-fold: true
#| code-summary: "Show the code"
data.frame(
  Coefficient = sort(dbcSIM2$coefficients[1629]
                     , decreasing = TRUE
  )
)
```


In contrast, the model was unable to predict the values where the outliers are (i.e. Woodlands Checkpoint to Kranji MRT Station, Woodlands Checkpoint to Woodlands Regional Bus Interchange, Choa Chu Kang North to Yew Tee Station).



# Reference

epsg.io (2023). EPSG: 3414 SVY21 / Singapore TM. https://epsg.io/3414

Kam, T. S. (2023). 15 Processing and Visualising Flow Data. *R for Geospatial Data Science and Analytics.* https://r4gdsa.netlify.app/chap15

Kam, T. S. (2023). 16 Calibrating Spatial Interaction Models with R. *R for Geospatial Data Science and Analytics.* https://r4gdsa.netlify.app/chap16

Kam, T. S. (2023). In-class Exercise 4: Preparing Spatial Interaction Modelling Variables. *ISSS624.* https://isss624.netlify.app/in-class_ex/in-class_ex4/in-class_ex4-gds

Kam, T. S. (2023). In-class Exercise 4: Calibrating Spatial Interaction Models with R. *ISSS624.* https://isss624.netlify.app/in-class_ex/in-class_ex4/in-class_ex4-sims

Miller, E. J. (2021). Traffic Analysis Zone Definition: Issues & Guidance. *Travel Modelling Group.* https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf

SingStat. (2022). Statistics on resident households are compiled by the Singapore Department of Statistics. *Households.* https://www.singstat.gov.sg/find-data/search-by-theme/households/households/latest-data

